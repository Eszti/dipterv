{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Find universal embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "def execute_notebook(nbfile):    \n",
    "    with io.open(nbfile) as f:\n",
    "        nb = current.read(f, 'json')\n",
    "    \n",
    "    ip = get_ipython()\n",
    "    \n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type != 'code':\n",
    "            continue\n",
    "        ip.run_cell(cell.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "execute_notebook(\"functions.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load Eglish embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "en_swad_fn = '/home/eszti/data/panlex_swadesh/swadesh110/test/eng-000.txt'\n",
    "en_embed_fn = '/mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec'\n",
    "\n",
    "en_swad, en_emb = get_embedding(en_swad_fn, en_embed_fn)\n",
    "# print(swad)\n",
    "# print(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load German embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "de_swad_fn = '/home/eszti/data/panlex_swadesh/swadesh110/test/deu.txt'\n",
    "de_embed_fn = '/mnt/permanent/Language/Multi/FB/wiki.de/wiki.de.vec'\n",
    "\n",
    "de_swad, de_emb = get_embedding(de_swad_fn, de_embed_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what the similar words are in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dog',\n",
       " u'bite',\n",
       " u'louse',\n",
       " u'claw',\n",
       " u'bird',\n",
       " u'snake',\n",
       " u'eat',\n",
       " u'nose',\n",
       " u'hair',\n",
       " u'ear',\n",
       " u'fat',\n",
       " u'tooth',\n",
       " u'bone',\n",
       " u'fish',\n",
       " u'foot',\n",
       " u'tail',\n",
       " u'egg',\n",
       " u'cold',\n",
       " u'skin',\n",
       " u'hot',\n",
       " u'eye',\n",
       " u'feather',\n",
       " u'tongue',\n",
       " u'bark',\n",
       " u'mouth',\n",
       " u'flesh',\n",
       " u'little',\n",
       " u'drink',\n",
       " u'kill',\n",
       " u'mountain',\n",
       " u'stand',\n",
       " u'sleep',\n",
       " u'go',\n",
       " u'night',\n",
       " u'head',\n",
       " u'neck',\n",
       " u'big',\n",
       " u'hand',\n",
       " u'good',\n",
       " u'blood',\n",
       " u'heart',\n",
       " u'horn',\n",
       " u'swim',\n",
       " u'liver',\n",
       " u'human',\n",
       " u'stone',\n",
       " u'worm',\n",
       " u'hear',\n",
       " u'moon',\n",
       " u'say',\n",
       " u'you',\n",
       " u'sit',\n",
       " u'that',\n",
       " u'fire',\n",
       " u'black',\n",
       " u'know',\n",
       " u'tree',\n",
       " u'white',\n",
       " u'what',\n",
       " u'fly',\n",
       " u'come',\n",
       " u'red',\n",
       " u'breast',\n",
       " u'sand',\n",
       " u'we',\n",
       " u'yellow',\n",
       " u'short',\n",
       " u'adulterous',\n",
       " u'smoke',\n",
       " u'dry',\n",
       " u'name',\n",
       " u'male',\n",
       " u'long',\n",
       " u'not',\n",
       " u'heavy',\n",
       " u'knee',\n",
       " u'rain',\n",
       " u'root',\n",
       " u'star',\n",
       " u'thin',\n",
       " u'path',\n",
       " u'far',\n",
       " u'who',\n",
       " u'female',\n",
       " u'abdomen',\n",
       " u'wind',\n",
       " u'leaf',\n",
       " u'many',\n",
       " u'water',\n",
       " u'salt',\n",
       " u'this',\n",
       " u'year',\n",
       " u'die',\n",
       " u'two',\n",
       " u'all',\n",
       " u'lie',\n",
       " u'cloud',\n",
       " u'sun',\n",
       " u'earth',\n",
       " u'as',\n",
       " u'see',\n",
       " u'i',\n",
       " u'burn',\n",
       " u'give',\n",
       " u'near',\n",
       " u'round',\n",
       " u'seed',\n",
       " u'green',\n",
       " u'new',\n",
       " u'full']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, sims_en = get_corr(en_emb, en_swad)\n",
    "sims_en['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what the similar words are in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hund',\n",
       " u'mensch',\n",
       " u'schwanz',\n",
       " u'mann',\n",
       " u'beissen',\n",
       " u'schlange',\n",
       " u'frau',\n",
       " u'nase',\n",
       " u'vogel',\n",
       " u'schlafen',\n",
       " u'ohr',\n",
       " u'bauch',\n",
       " u'kralle',\n",
       " u'fisch',\n",
       " u'fleisch',\n",
       " u'knochen',\n",
       " u'kopf',\n",
       " u'haut',\n",
       " u'trinken',\n",
       " u'mund',\n",
       " u'haar',\n",
       " u'umbringen',\n",
       " u'zunge',\n",
       " u'leber',\n",
       " u'blut',\n",
       " u'nacken',\n",
       " u'brust',\n",
       " u'mond',\n",
       " u'stein',\n",
       " u'baum',\n",
       " u'wer',\n",
       " u'laus',\n",
       " u'sagen',\n",
       " u'sterben',\n",
       " u'wolke',\n",
       " u'knie',\n",
       " u'herz',\n",
       " u'gut',\n",
       " u'hand',\n",
       " u'auge',\n",
       " u'wurm',\n",
       " u'h\\xf6ren',\n",
       " u'was',\n",
       " u'fliegen',\n",
       " u'kennen',\n",
       " u'nicht',\n",
       " u'wir',\n",
       " u'viel',\n",
       " u'fuss',\n",
       " u'stern',\n",
       " u'nacht',\n",
       " u'ei',\n",
       " u'zahn',\n",
       " u'rauch',\n",
       " u'sehen',\n",
       " u'dick',\n",
       " u'ich',\n",
       " u'ein',\n",
       " u'weiss',\n",
       " u'essen',\n",
       " u'feder',\n",
       " u'du',\n",
       " u'schwer',\n",
       " u'wurzel',\n",
       " u'wasser',\n",
       " u'erde',\n",
       " u'schwimmen',\n",
       " u'sonne',\n",
       " u'borke',\n",
       " u'feuer',\n",
       " u'sitzen',\n",
       " u'pfad',\n",
       " u'wind',\n",
       " u'gehen',\n",
       " u'kalt',\n",
       " u'voll',\n",
       " u'schwarz',\n",
       " u'berg',\n",
       " u'regen',\n",
       " u'gross',\n",
       " u'brennen',\n",
       " u'klein',\n",
       " u'geben',\n",
       " u'warm',\n",
       " u'das',\n",
       " u'horn',\n",
       " u'nah',\n",
       " u'salz',\n",
       " u'name',\n",
       " u'lang',\n",
       " u'zwei',\n",
       " u'asche',\n",
       " u'jahr',\n",
       " u'samen',\n",
       " u'rot',\n",
       " u'sand',\n",
       " u'trocken',\n",
       " u'blatt',\n",
       " u'kurz',\n",
       " u'stehen',\n",
       " u'kommen',\n",
       " u'rund',\n",
       " u'dieses',\n",
       " u'all',\n",
       " u'gr\\xfcn',\n",
       " u'd\\xfcnn',\n",
       " u'weit',\n",
       " u'gelb',\n",
       " u'liegen',\n",
       " u'neu']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, sims_de = get_corr(de_emb, de_swad)\n",
    "sims_de['hund']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train universal embedding based on English and German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 388.830505\n",
      "Loss at step 100: 380.949127\n",
      "Loss at step 200: 373.654785\n",
      "Loss at step 300: 366.869934\n",
      "Loss at step 400: 360.520569\n",
      "Loss at step 500: 354.539001\n",
      "Loss at step 600: 348.864502\n",
      "Loss at step 700: 343.444885\n",
      "Loss at step 800: 338.234863\n",
      "Loss at step 900: 333.196655\n",
      "Loss at step 1000: 328.299011\n",
      "Loss at step 1100: 323.516724\n",
      "Loss at step 1200: 318.828735\n",
      "Loss at step 1300: 314.218872\n",
      "Loss at step 1400: 309.674255\n",
      "Loss at step 1500: 305.184692\n",
      "Loss at step 1600: 300.741821\n",
      "Loss at step 1700: 296.340271\n",
      "Loss at step 1800: 291.975220\n",
      "Loss at step 1900: 287.642975\n",
      "Loss at step 2000: 283.341309\n",
      "Loss at step 2100: 279.068329\n",
      "Loss at step 2200: 274.822906\n",
      "Loss at step 2300: 270.603760\n",
      "Loss at step 2400: 266.411011\n",
      "Loss at step 2500: 262.244141\n",
      "Loss at step 2600: 258.103119\n",
      "Loss at step 2700: 253.988129\n",
      "Loss at step 2800: 249.899414\n",
      "Loss at step 2900: 245.837555\n",
      "Loss at step 3000: 241.802490\n",
      "Loss at step 3100: 237.795288\n",
      "Loss at step 3200: 233.816010\n",
      "Loss at step 3300: 229.865570\n",
      "Loss at step 3400: 225.944580\n",
      "Loss at step 3500: 222.053528\n",
      "Loss at step 3600: 218.193237\n",
      "Loss at step 3700: 214.364563\n",
      "Loss at step 3800: 210.568115\n",
      "Loss at step 3900: 206.804672\n",
      "Loss at step 4000: 203.075378\n",
      "Loss at step 4100: 199.380707\n",
      "Loss at step 4200: 195.721573\n",
      "Loss at step 4300: 192.099335\n",
      "Loss at step 4400: 188.514526\n",
      "Loss at step 4500: 184.968323\n",
      "Loss at step 4600: 181.461639\n",
      "Loss at step 4700: 177.995605\n",
      "Loss at step 4800: 174.571167\n",
      "Loss at step 4900: 171.189529\n",
      "Loss at step 5000: 167.851730\n",
      "Loss at step 5100: 164.558960\n",
      "Loss at step 5200: 161.312256\n",
      "Loss at step 5300: 158.112885\n",
      "Loss at step 5400: 154.961929\n",
      "Loss at step 5500: 151.860626\n",
      "Loss at step 5600: 148.810150\n",
      "Loss at step 5700: 145.811646\n",
      "Loss at step 5800: 142.866241\n",
      "Loss at step 5900: 139.975128\n",
      "Loss at step 6000: 137.139252\n",
      "Loss at step 6100: 134.359894\n",
      "Loss at step 6200: 131.638062\n",
      "Loss at step 6300: 128.974686\n",
      "Loss at step 6400: 126.370667\n",
      "Loss at step 6500: 123.826897\n",
      "Loss at step 6600: 121.344086\n",
      "Loss at step 6700: 118.922958\n",
      "Loss at step 6800: 116.564133\n",
      "Loss at step 6900: 114.267952\n",
      "Loss at step 7000: 112.034851\n",
      "Loss at step 7100: 109.865021\n",
      "Loss at step 7200: 107.758507\n",
      "Loss at step 7300: 105.715317\n",
      "Loss at step 7400: 103.735138\n",
      "Loss at step 7500: 101.817696\n",
      "Loss at step 7600: 99.962402\n",
      "Loss at step 7700: 98.168579\n",
      "Loss at step 7800: 96.435432\n",
      "Loss at step 7900: 94.761971\n",
      "Loss at step 8000: 93.147064\n",
      "Loss at step 8100: 91.589455\n",
      "Loss at step 8200: 90.087753\n",
      "Loss at step 8300: 88.640427\n",
      "Loss at step 8400: 87.245911\n",
      "Loss at step 8500: 85.902481\n",
      "Loss at step 8600: 84.608307\n",
      "Loss at step 8700: 83.361618\n",
      "Loss at step 8800: 82.160431\n",
      "Loss at step 8900: 81.002876\n",
      "Loss at step 9000: 79.886971\n",
      "Loss at step 9100: 78.810707\n",
      "Loss at step 9200: 77.772224\n",
      "Loss at step 9300: 76.769516\n",
      "Loss at step 9400: 75.800697\n",
      "Loss at step 9500: 74.863922\n",
      "Loss at step 9600: 73.957283\n",
      "Loss at step 9700: 73.079086\n",
      "Loss at step 9800: 72.227600\n",
      "Loss at step 9900: 71.401199\n",
      "Loss at step 10000: 70.598358\n",
      "Loss at step 10100: 69.817505\n",
      "Loss at step 10200: 69.057281\n",
      "Loss at step 10300: 68.316330\n",
      "Loss at step 10400: 67.593437\n",
      "Loss at step 10500: 66.887375\n",
      "Loss at step 10600: 66.197029\n",
      "Loss at step 10700: 65.521408\n",
      "Loss at step 10800: 64.859573\n",
      "Loss at step 10900: 64.210571\n",
      "Loss at step 11000: 63.573631\n",
      "Loss at step 11100: 62.948002\n",
      "Loss at step 11200: 62.332939\n",
      "Loss at step 11300: 61.727772\n",
      "Loss at step 11400: 61.131989\n",
      "Loss at step 11500: 60.545052\n",
      "Loss at step 11600: 59.966412\n",
      "Loss at step 11700: 59.395638\n",
      "Loss at step 11800: 58.832348\n",
      "Loss at step 11900: 58.276157\n",
      "Loss at step 12000: 57.726738\n",
      "Loss at step 12100: 57.183773\n",
      "Loss at step 12200: 56.646996\n",
      "Loss at step 12300: 56.116180\n",
      "Loss at step 12400: 55.591007\n",
      "Loss at step 12500: 55.071434\n",
      "Loss at step 12600: 54.557159\n",
      "Loss at step 12700: 54.048035\n",
      "Loss at step 12800: 53.543915\n",
      "Loss at step 12900: 53.044758\n",
      "Loss at step 13000: 52.550278\n",
      "Loss at step 13100: 52.060490\n",
      "Loss at step 13200: 51.575245\n",
      "Loss at step 13300: 51.094490\n",
      "Loss at step 13400: 50.618107\n",
      "Loss at step 13500: 50.146034\n",
      "Loss at step 13600: 49.678207\n",
      "Loss at step 13700: 49.214527\n",
      "Loss at step 13800: 48.755016\n",
      "Loss at step 13900: 48.299534\n",
      "Loss at step 14000: 47.848026\n",
      "Loss at step 14100: 47.400551\n",
      "Loss at step 14200: 46.956928\n",
      "Loss at step 14300: 46.517197\n",
      "Loss at step 14400: 46.081272\n",
      "Loss at step 14500: 45.715336\n",
      "Loss at step 14600: 45.387901\n",
      "Loss at step 14700: 45.074299\n",
      "Loss at step 14800: 44.761879\n",
      "Loss at step 14900: 44.450649\n",
      "Loss at step 15000: 44.140575\n",
      "Loss at step 15100: 43.831680\n",
      "Loss at step 15200: 43.523945\n",
      "Loss at step 15300: 43.217434\n",
      "Loss at step 15400: 42.912094\n",
      "Loss at step 15500: 42.607922\n",
      "Loss at step 15600: 42.304966\n",
      "Loss at step 15700: 42.003139\n",
      "Loss at step 15800: 41.702579\n",
      "Loss at step 15900: 41.403175\n",
      "Loss at step 16000: 41.104965\n",
      "Loss at step 16100: 40.807964\n",
      "Loss at step 16200: 40.512119\n",
      "Loss at step 16300: 40.217537\n",
      "Loss at step 16400: 39.924129\n",
      "Loss at step 16500: 39.631908\n",
      "Loss at step 16600: 39.340889\n",
      "Loss at step 16700: 39.051117\n",
      "Loss at step 16800: 38.762539\n",
      "Loss at step 16900: 38.475174\n",
      "Loss at step 17000: 38.189007\n",
      "Loss at step 17100: 37.904049\n",
      "Loss at step 17200: 37.620346\n",
      "Loss at step 17300: 37.337826\n",
      "Loss at step 17400: 37.056519\n",
      "Loss at step 17500: 36.776447\n",
      "Loss at step 17600: 36.497585\n",
      "Loss at step 17700: 36.219952\n",
      "Loss at step 17800: 35.943535\n",
      "Loss at step 17900: 35.668327\n",
      "Loss at step 18000: 35.394341\n",
      "Loss at step 18100: 35.121593\n",
      "Loss at step 18200: 34.850060\n",
      "Loss at step 18300: 34.579773\n",
      "Loss at step 18400: 34.310726\n",
      "Loss at step 18500: 34.042870\n",
      "Loss at step 18600: 33.776253\n",
      "Loss at step 18700: 33.510857\n",
      "Loss at step 18800: 33.246708\n",
      "Loss at step 18900: 32.983761\n",
      "Loss at step 19000: 32.722076\n",
      "Loss at step 19100: 32.461590\n",
      "Loss at step 19200: 32.202362\n",
      "Loss at step 19300: 31.944376\n",
      "Loss at step 19400: 31.687567\n",
      "Loss at step 19500: 31.432022\n",
      "Loss at step 19600: 31.177704\n",
      "Loss at step 19700: 30.924606\n",
      "Loss at step 19800: 30.672752\n",
      "Loss at step 19900: 30.422132\n",
      "Loss at step 20000: 30.172724\n",
      "Loss at step 20100: 29.924543\n",
      "Loss at step 20200: 29.677601\n",
      "Loss at step 20300: 29.431852\n",
      "Loss at step 20400: 29.187401\n",
      "Loss at step 20500: 28.944126\n",
      "Loss at step 20600: 28.702089\n",
      "Loss at step 20700: 28.461277\n",
      "Loss at step 20800: 28.221676\n",
      "Loss at step 20900: 27.983303\n",
      "Loss at step 21000: 27.746140\n",
      "Loss at step 21100: 27.510223\n",
      "Loss at step 21200: 27.275517\n",
      "Loss at step 21300: 27.042013\n",
      "Loss at step 21400: 26.809734\n",
      "Loss at step 21500: 26.578688\n",
      "Loss at step 21600: 26.348839\n",
      "Loss at step 21700: 26.120190\n",
      "Loss at step 21800: 25.892759\n",
      "Loss at step 21900: 25.666538\n",
      "Loss at step 22000: 25.441507\n",
      "Loss at step 22100: 25.217705\n",
      "Loss at step 22200: 24.995071\n",
      "Loss at step 22300: 24.773651\n",
      "Loss at step 22400: 24.553459\n",
      "Loss at step 22500: 24.334423\n",
      "Loss at step 22600: 24.116589\n",
      "Loss at step 22700: 23.899969\n",
      "Loss at step 22800: 23.684513\n",
      "Loss at step 22900: 23.470240\n",
      "Loss at step 23000: 23.257166\n",
      "Loss at step 23100: 23.045252\n",
      "Loss at step 23200: 22.834509\n",
      "Loss at step 23300: 22.624968\n",
      "Loss at step 23400: 22.416578\n",
      "Loss at step 23500: 22.209366\n",
      "Loss at step 23600: 22.003309\n",
      "Loss at step 23700: 21.798410\n",
      "Loss at step 23800: 21.594679\n",
      "Loss at step 23900: 21.392101\n",
      "Loss at step 24000: 21.190678\n",
      "Loss at step 24100: 20.990374\n",
      "Loss at step 24200: 20.791252\n",
      "Loss at step 24300: 20.593254\n",
      "Loss at step 24400: 20.396385\n",
      "Loss at step 24500: 20.200657\n",
      "Loss at step 24600: 20.006063\n",
      "Loss at step 24700: 19.812584\n",
      "Loss at step 24800: 19.620226\n",
      "Loss at step 24900: 19.428989\n",
      "Loss at step 25000: 19.238863\n",
      "Loss at step 25100: 19.049854\n",
      "Loss at step 25200: 18.861933\n",
      "Loss at step 25300: 18.675131\n",
      "Loss at step 25400: 18.489395\n",
      "Loss at step 25500: 18.304779\n",
      "Loss at step 25600: 18.121244\n",
      "Loss at step 25700: 17.938778\n",
      "Loss at step 25800: 17.757389\n",
      "Loss at step 25900: 17.577080\n",
      "Loss at step 26000: 17.397840\n",
      "Loss at step 26100: 17.219664\n",
      "Loss at step 26200: 17.042549\n",
      "Loss at step 26300: 16.866474\n",
      "Loss at step 26400: 16.691448\n",
      "Loss at step 26500: 16.517481\n",
      "Loss at step 26600: 16.344549\n",
      "Loss at step 26700: 16.172646\n",
      "Loss at step 26800: 16.001781\n",
      "Loss at step 26900: 15.831923\n",
      "Loss at step 27000: 15.663094\n",
      "Loss at step 27100: 15.495270\n",
      "Loss at step 27200: 15.328469\n",
      "Loss at step 27300: 15.162657\n",
      "Loss at step 27400: 14.997853\n",
      "Loss at step 27500: 14.834042\n",
      "Loss at step 27600: 14.671196\n",
      "Loss at step 27700: 14.509356\n",
      "Loss at step 27800: 14.348479\n",
      "Loss at step 27900: 14.188572\n",
      "Loss at step 28000: 14.029643\n",
      "Loss at step 28100: 13.871667\n",
      "Loss at step 28200: 13.714648\n",
      "Loss at step 28300: 13.558575\n",
      "Loss at step 28400: 13.403450\n",
      "Loss at step 28500: 13.249260\n",
      "Loss at step 28600: 13.096014\n",
      "Loss at step 28700: 12.943683\n",
      "Loss at step 28800: 12.792279\n",
      "Loss at step 28900: 12.641792\n",
      "Loss at step 29000: 12.492218\n",
      "Loss at step 29100: 12.343547\n",
      "Loss at step 29200: 12.195777\n",
      "Loss at step 29300: 12.048912\n",
      "Loss at step 29400: 11.902928\n",
      "Loss at step 29500: 11.757833\n",
      "Loss at step 29600: 11.613608\n",
      "Loss at step 29700: 11.470268\n",
      "Loss at step 29800: 11.327797\n",
      "Loss at step 29900: 11.186188\n",
      "Loss at step 30000: 11.045427\n",
      "Loss at step 30100: 10.905533\n",
      "Loss at step 30200: 10.766479\n",
      "Loss at step 30300: 10.628268\n",
      "Loss at step 30400: 10.490894\n",
      "Loss at step 30500: 10.354361\n",
      "Loss at step 30600: 10.218644\n",
      "Loss at step 30700: 10.083754\n",
      "Loss at step 30800: 9.949686\n",
      "Loss at step 30900: 9.816424\n",
      "Loss at step 31000: 9.683967\n",
      "Loss at step 31100: 9.552324\n",
      "Loss at step 31200: 9.421460\n",
      "Loss at step 31300: 9.291398\n",
      "Loss at step 31400: 9.162132\n",
      "Loss at step 31500: 9.033647\n",
      "Loss at step 31600: 8.905928\n",
      "Loss at step 31700: 8.778997\n",
      "Loss at step 31800: 8.652826\n",
      "Loss at step 31900: 8.527423\n",
      "Loss at step 32000: 8.402778\n",
      "Loss at step 32100: 8.278891\n",
      "Loss at step 32200: 8.155750\n",
      "Loss at step 32300: 8.033357\n",
      "Loss at step 32400: 7.911709\n",
      "Loss at step 32500: 7.790792\n",
      "Loss at step 32600: 7.670616\n",
      "Loss at step 32700: 7.551170\n",
      "Loss at step 32800: 7.432448\n",
      "Loss at step 32900: 7.314447\n",
      "Loss at step 33000: 7.197166\n",
      "Loss at step 33100: 7.080592\n",
      "Loss at step 33200: 6.964734\n",
      "Loss at step 33300: 6.849579\n",
      "Loss at step 33400: 6.735125\n",
      "Loss at step 33500: 6.621371\n",
      "Loss at step 33600: 6.508313\n",
      "Loss at step 33700: 6.395936\n",
      "Loss at step 33800: 6.284261\n",
      "Loss at step 33900: 6.173261\n",
      "Loss at step 34000: 6.062943\n",
      "Loss at step 34100: 5.953300\n",
      "Loss at step 34200: 5.844335\n",
      "Loss at step 34300: 5.736038\n",
      "Loss at step 34400: 5.628411\n",
      "Loss at step 34500: 5.521447\n",
      "Loss at step 34600: 5.415143\n",
      "Loss at step 34700: 5.309497\n",
      "Loss at step 34800: 5.204503\n",
      "Loss at step 34900: 5.100163\n",
      "Loss at step 35000: 4.996475\n",
      "Loss at step 35100: 4.893430\n",
      "Loss at step 35200: 4.791030\n",
      "Loss at step 35300: 4.689273\n",
      "Loss at step 35400: 4.588146\n",
      "Loss at step 35500: 4.487665\n",
      "Loss at step 35600: 4.387814\n",
      "Loss at step 35700: 4.288589\n",
      "Loss at step 35800: 4.189999\n",
      "Loss at step 35900: 4.092032\n",
      "Loss at step 36000: 3.994690\n",
      "Loss at step 36100: 3.897969\n",
      "Loss at step 36200: 3.801871\n",
      "Loss at step 36300: 3.706386\n",
      "Loss at step 36400: 3.611518\n",
      "Loss at step 36500: 3.517264\n",
      "Loss at step 36600: 3.423624\n",
      "Loss at step 36700: 3.330594\n",
      "Loss at step 36800: 3.238175\n",
      "Loss at step 36900: 3.146358\n",
      "Loss at step 37000: 3.055150\n",
      "Loss at step 37100: 2.964546\n",
      "Loss at step 37200: 2.874547\n",
      "Loss at step 37300: 2.785150\n",
      "Loss at step 37400: 2.696352\n",
      "Loss at step 37500: 2.608158\n",
      "Loss at step 37600: 2.520561\n",
      "Loss at step 37700: 2.433564\n",
      "Loss at step 37800: 2.347168\n",
      "Loss at step 37900: 2.261368\n",
      "Loss at step 38000: 2.176168\n",
      "Loss at step 38100: 2.091568\n",
      "Loss at step 38200: 2.007565\n",
      "Loss at step 38300: 1.924164\n",
      "Loss at step 38400: 1.841361\n",
      "Loss at step 38500: 1.759161\n",
      "Loss at step 38600: 1.677566\n",
      "Loss at step 38700: 1.596575\n",
      "Loss at step 38800: 1.516190\n",
      "Loss at step 38900: 1.436416\n",
      "Loss at step 39000: 1.357257\n",
      "Loss at step 39100: 1.278712\n",
      "Loss at step 39200: 1.200789\n",
      "Loss at step 39300: 1.123493\n",
      "Loss at step 39400: 1.046828\n",
      "Loss at step 39500: 0.970802\n",
      "Loss at step 39600: 0.895423\n",
      "Loss at step 39700: 0.820700\n",
      "Loss at step 39800: 0.746645\n",
      "Loss at step 39900: 0.673270\n",
      "Loss at step 40000: 0.600590\n",
      "Loss at step 40100: 0.528624\n",
      "Loss at step 40200: 0.457397\n",
      "Loss at step 40300: 0.386940\n",
      "Loss at step 40400: 0.317292\n",
      "Loss at step 40500: 0.248523\n",
      "Loss at step 40600: 0.180902\n",
      "Loss at step 40700: 0.151559\n",
      "Loss at step 40800: 0.151856\n",
      "Loss at step 40900: 0.151915\n",
      "Loss at step 41000: 0.151934\n",
      "Loss at step 41100: 0.151941\n",
      "Loss at step 41200: 0.151944\n",
      "Loss at step 41300: 0.151946\n",
      "Loss at step 41400: 0.151948\n",
      "Loss at step 41500: 0.151946\n",
      "Loss at step 41600: 0.151945\n",
      "Loss at step 41700: 0.151944\n",
      "Loss at step 41800: 0.151941\n",
      "Loss at step 41900: 0.151936\n",
      "Loss at step 42000: 0.151935\n",
      "Loss at step 42100: 0.151936\n",
      "Loss at step 42200: 0.151935\n",
      "Loss at step 42300: 0.151936\n",
      "Loss at step 42400: 0.151935\n",
      "Loss at step 42500: 0.151933\n",
      "Loss at step 42600: 0.151933\n",
      "Loss at step 42700: 0.151931\n",
      "Loss at step 42800: 0.151932\n",
      "Loss at step 42900: 0.151930\n",
      "Loss at step 43000: 0.151929\n",
      "Loss at step 43100: 0.151929\n",
      "Loss at step 43200: 0.151926\n",
      "Loss at step 43300: 0.151927\n",
      "Loss at step 43400: 0.151928\n",
      "Loss at step 43500: 0.151928\n",
      "Loss at step 43600: 0.151929\n",
      "Loss at step 43700: 0.151931\n",
      "Loss at step 43800: 0.151931\n",
      "Loss at step 43900: 0.151933\n",
      "Loss at step 44000: 0.151935\n",
      "Loss at step 44100: 0.151936\n",
      "Loss at step 44200: 0.151935\n",
      "Loss at step 44300: 0.151934\n",
      "Loss at step 44400: 0.151935\n",
      "Loss at step 44500: 0.151936\n",
      "Loss at step 44600: 0.151937\n",
      "Loss at step 44700: 0.151938\n",
      "Loss at step 44800: 0.151939\n",
      "Loss at step 44900: 0.151939\n",
      "Loss at step 45000: 0.151939\n",
      "Loss at step 45100: 0.151938\n",
      "Loss at step 45200: 0.151936\n",
      "Loss at step 45300: 0.151937\n",
      "Loss at step 45400: 0.151937\n",
      "Loss at step 45500: 0.151939\n",
      "Loss at step 45600: 0.151940\n",
      "Loss at step 45700: 0.151942\n",
      "Loss at step 45800: 0.151943\n",
      "Loss at step 45900: 0.151944\n",
      "Loss at step 46000: 0.151944\n",
      "Loss at step 46100: 0.151946\n",
      "Loss at step 46200: 0.151946\n",
      "Loss at step 46300: 0.151944\n",
      "Loss at step 46400: 0.151941\n",
      "Loss at step 46500: 0.151943\n",
      "Loss at step 46600: 0.151944\n",
      "Loss at step 46700: 0.151944\n",
      "Loss at step 46800: 0.151944\n",
      "Loss at step 46900: 0.151944\n",
      "Loss at step 47000: 0.151945\n",
      "Loss at step 47100: 0.151944\n",
      "Loss at step 47200: 0.151942\n",
      "Loss at step 47300: 0.151941\n",
      "Loss at step 47400: 0.151942\n",
      "Loss at step 47500: 0.151942\n",
      "Loss at step 47600: 0.151940\n",
      "Loss at step 47700: 0.151941\n",
      "Loss at step 47800: 0.151941\n",
      "Loss at step 47900: 0.151941\n",
      "Loss at step 48000: 0.151941\n",
      "Loss at step 48100: 0.151942\n",
      "Loss at step 48200: 0.151944\n",
      "Loss at step 48300: 0.151944\n",
      "Loss at step 48400: 0.151943\n",
      "Loss at step 48500: 0.151943\n",
      "Loss at step 48600: 0.151941\n",
      "Loss at step 48700: 0.151942\n",
      "Loss at step 48800: 0.151943\n",
      "Loss at step 48900: 0.151943\n",
      "Loss at step 49000: 0.151944\n",
      "Loss at step 49100: 0.151942\n",
      "Loss at step 49200: 0.151942\n",
      "Loss at step 49300: 0.151943\n",
      "Loss at step 49400: 0.151943\n",
      "Loss at step 49500: 0.151945\n",
      "Loss at step 49600: 0.151945\n",
      "Loss at step 49700: 0.151946\n",
      "Loss at step 49800: 0.151949\n",
      "Loss at step 49900: 0.151952\n",
      "\n",
      "\n",
      "Transform 1:\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Transform 2:\n",
      "[[ 0.43068695 -0.26140624  1.02249229 ..., -1.18112159 -0.40523016\n",
      "   0.77026612]\n",
      " [ 1.15509498  0.43282536  1.14364088 ...,  0.46049604 -1.17955983\n",
      "  -0.84312451]\n",
      " [-0.25501913  0.58726317  0.04809749 ...,  0.76116157  0.79628122\n",
      "  -0.06527986]\n",
      " ..., \n",
      " [ 0.44978642  0.06734851  0.13047731 ...,  0.18447761  0.97385496\n",
      "  -0.4205178 ]\n",
      " [ 1.13428032 -0.32366571  0.73028576 ..., -0.49014655 -1.03708553\n",
      "   0.45603162]\n",
      " [ 1.65531528 -0.43407655  0.27551943 ..., -1.46717119 -0.66937047\n",
      "  -0.16815561]]\n",
      "Universal embedding:\n",
      "[[ -1.62593916e-01   3.39183561e-03  -7.69158080e-02 ...,  -6.64658323e-02\n",
      "    4.98440340e-02  -1.08556692e-02]\n",
      " [ -4.36387360e-02  -1.49105554e-02  -4.34577614e-02 ...,  -3.63268494e-03\n",
      "   -3.60850026e-05   5.56133837e-02]\n",
      " [ -8.15020651e-02  -2.40627322e-02  -3.50313485e-02 ...,   1.91829503e-02\n",
      "    4.07609232e-02   5.57510853e-02]\n",
      " ..., \n",
      " [ -1.28634363e-01  -6.19927468e-03  -2.86452826e-02 ...,   3.84307466e-02\n",
      "    1.30310223e-01   2.87203211e-02]\n",
      " [ -4.97821830e-02  -5.56418002e-02  -4.76733856e-02 ...,   9.64846909e-02\n",
      "   -2.92948470e-03  -1.14370482e-02]\n",
      " [ -3.28783989e-02  -5.84800765e-02   8.13241601e-02 ...,  -3.10161691e-02\n",
      "    3.40169743e-02  -1.34242820e-02]]\n",
      "\n",
      "\n",
      "W1*T1:\n",
      "[[ -1.62625358e-01   3.36121395e-03  -7.69655257e-02 ...,  -6.64392561e-02\n",
      "    4.98530976e-02  -1.08726015e-02]\n",
      " [ -4.36824225e-02  -1.49530806e-02  -4.35268208e-02 ...,  -3.59577429e-03\n",
      "   -2.35032567e-05   5.55898510e-02]\n",
      " [ -8.15486684e-02  -2.41080951e-02  -3.51050124e-02 ...,   1.92223284e-02\n",
      "    4.07743342e-02   5.57259880e-02]\n",
      " ..., \n",
      " [ -1.28676072e-01  -6.23986265e-03  -2.87111904e-02 ...,   3.84659730e-02\n",
      "    1.30322233e-01   2.86978595e-02]\n",
      " [ -4.98176925e-02  -5.56763671e-02  -4.77295145e-02 ...,   9.65146795e-02\n",
      "   -2.91924528e-03  -1.14561766e-02]\n",
      " [ -3.29067893e-02  -5.85077107e-02   8.12792853e-02 ...,  -3.09921913e-02\n",
      "    3.40251587e-02  -1.34395724e-02]]\n",
      "W2*T2:\n",
      "[[ -1.63040161e-01   2.95738876e-03  -7.76213109e-02 ...,  -6.60885721e-02\n",
      "    4.99726869e-02  -1.10959550e-02]\n",
      " [ -4.42588478e-02  -1.55139696e-02  -4.44377214e-02 ...,  -3.10879201e-03\n",
      "    1.42429024e-04   5.52794002e-02]\n",
      " [ -8.21633488e-02  -2.47065611e-02  -3.60766985e-02 ...,   1.97418779e-02\n",
      "    4.09511626e-02   5.53950146e-02]\n",
      " ..., \n",
      " [ -1.29226208e-01  -6.77526742e-03  -2.95806155e-02 ...,   3.89306843e-02\n",
      "    1.30480587e-01   2.84014791e-02]\n",
      " [ -5.02861440e-02  -5.61322570e-02  -4.84698527e-02 ...,   9.69103426e-02\n",
      "   -2.78423727e-03  -1.17085297e-02]\n",
      " [ -3.32812816e-02  -5.88723496e-02   8.06873515e-02 ...,  -3.06758434e-02\n",
      "    3.41331512e-02  -1.36411823e-02]]\n"
     ]
    }
   ],
   "source": [
    "    W = np.ndarray(shape=(2, len(en_swad), en_emb.shape[1]), dtype=np.float32)\n",
    "    W[0, :, :] = en_emb\n",
    "    W[1, :, :] = de_emb\n",
    "    T1, T, A = train(W, num_steps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_mx, sim_corr, sims_univ = get_corr(A, en_swad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.27099049,  0.14395706, ...,  0.22068045,\n",
       "         0.17661083,  0.25888366],\n",
       "       [ 0.27099049,  1.        ,  0.13455899, ...,  0.1125395 ,\n",
       "         0.15868136,  0.25678423],\n",
       "       [ 0.14395706,  0.13455899,  1.00000012, ...,  0.2748999 ,\n",
       "         0.28643367,  0.09522829],\n",
       "       ..., \n",
       "       [ 0.22068045,  0.1125395 ,  0.2748999 , ...,  1.00000012,\n",
       "         0.23129509,  0.1567193 ],\n",
       "       [ 0.17661083,  0.15868136,  0.28643367, ...,  0.23129509,\n",
       "         1.00000012,  0.12602587],\n",
       "       [ 0.25888366,  0.25678423,  0.09522829, ...,  0.1567193 ,\n",
       "         0.12602587,  1.00000012]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what the similar words are in the universal embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dog',\n",
       " u'bite',\n",
       " u'louse',\n",
       " u'claw',\n",
       " u'bird',\n",
       " u'snake',\n",
       " u'eat',\n",
       " u'nose',\n",
       " u'hair',\n",
       " u'ear',\n",
       " u'fat',\n",
       " u'tooth',\n",
       " u'bone',\n",
       " u'fish',\n",
       " u'foot',\n",
       " u'tail',\n",
       " u'egg',\n",
       " u'cold',\n",
       " u'skin',\n",
       " u'hot',\n",
       " u'eye',\n",
       " u'feather',\n",
       " u'tongue',\n",
       " u'bark',\n",
       " u'mouth',\n",
       " u'flesh',\n",
       " u'little',\n",
       " u'drink',\n",
       " u'kill',\n",
       " u'mountain',\n",
       " u'stand',\n",
       " u'sleep',\n",
       " u'go',\n",
       " u'night',\n",
       " u'head',\n",
       " u'neck',\n",
       " u'big',\n",
       " u'hand',\n",
       " u'good',\n",
       " u'blood',\n",
       " u'heart',\n",
       " u'horn',\n",
       " u'swim',\n",
       " u'liver',\n",
       " u'human',\n",
       " u'stone',\n",
       " u'worm',\n",
       " u'hear',\n",
       " u'moon',\n",
       " u'say',\n",
       " u'you',\n",
       " u'sit',\n",
       " u'that',\n",
       " u'fire',\n",
       " u'black',\n",
       " u'know',\n",
       " u'tree',\n",
       " u'white',\n",
       " u'what',\n",
       " u'fly',\n",
       " u'come',\n",
       " u'red',\n",
       " u'breast',\n",
       " u'sand',\n",
       " u'we',\n",
       " u'yellow',\n",
       " u'short',\n",
       " u'adulterous',\n",
       " u'smoke',\n",
       " u'dry',\n",
       " u'name',\n",
       " u'male',\n",
       " u'long',\n",
       " u'not',\n",
       " u'heavy',\n",
       " u'knee',\n",
       " u'rain',\n",
       " u'root',\n",
       " u'star',\n",
       " u'thin',\n",
       " u'path',\n",
       " u'far',\n",
       " u'who',\n",
       " u'female',\n",
       " u'abdomen',\n",
       " u'wind',\n",
       " u'leaf',\n",
       " u'many',\n",
       " u'water',\n",
       " u'salt',\n",
       " u'this',\n",
       " u'year',\n",
       " u'die',\n",
       " u'two',\n",
       " u'all',\n",
       " u'lie',\n",
       " u'cloud',\n",
       " u'sun',\n",
       " u'earth',\n",
       " u'as',\n",
       " u'see',\n",
       " u'i',\n",
       " u'burn',\n",
       " u'give',\n",
       " u'near',\n",
       " u'round',\n",
       " u'seed',\n",
       " u'green',\n",
       " u'new',\n",
       " u'full']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_univ['dog']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
