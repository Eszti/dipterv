{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Find universal embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from nbformat import current\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "def execute_notebook(nbfile):    \n",
    "    with io.open(nbfile) as f:\n",
    "        nb = current.read(f, 'json')\n",
    "    \n",
    "    ip = get_ipython()\n",
    "    \n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type != 'code':\n",
    "            continue\n",
    "        ip.run_cell(cell.input)\n",
    "        \n",
    "execute_notebook(\"functions.ipynb\")        \n",
    "swad_idx = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load Eglish embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'all', u'as', u'bark', u'abdomen', u'big', u'bird', u'bite', u'black', u'blood', u'bone', u'breast', u'burn', u'claw', u'cloud', u'cold', u'come', u'die', u'dog', u'drink', u'dry', u'ear', u'earth', u'eat', u'egg', u'eye', u'fat', u'feather', u'fire', u'fish', u'fly', u'foot', u'full', u'give', u'good', u'green', u'hair', u'hand', u'head', u'hear', u'heart', u'horn', u'i', u'kill', u'knee', u'know', u'leaf', u'lie', u'liver', u'long', u'louse', u'male', u'many', u'flesh', u'moon', u'mountain', u'mouth', u'name', u'neck', u'new', u'night', u'nose', u'not', u'adulterous', u'human', u'rain', u'red', u'path', u'root', u'round', u'sand', u'say', u'see', u'seed', u'sit', u'skin', u'sleep', u'little', u'smoke', u'stand', u'star', u'stone', u'sun', u'swim', u'tail', u'that', u'this', u'you', u'tongue', u'tooth', u'tree', u'two', u'go', u'hot', u'water', u'we', u'what', u'white', u'who', u'female', u'yellow', u'far', u'heavy', u'near', u'salt', u'short', u'snake', u'thin', u'wind', u'worm', u'year']\n",
      "[[ -1.62625358e-01   3.36121395e-03  -7.69655257e-02 ...,  -6.64392561e-02\n",
      "    4.98530976e-02  -1.08726015e-02]\n",
      " [ -4.36824225e-02  -1.49530806e-02  -4.35268208e-02 ...,  -3.59577429e-03\n",
      "   -2.35032567e-05   5.55898510e-02]\n",
      " [ -8.15486684e-02  -2.41080951e-02  -3.51050124e-02 ...,   1.92223284e-02\n",
      "    4.07743342e-02   5.57259880e-02]\n",
      " ..., \n",
      " [ -1.28676072e-01  -6.23986265e-03  -2.87111904e-02 ...,   3.84659730e-02\n",
      "    1.30322233e-01   2.86978595e-02]\n",
      " [ -4.98176925e-02  -5.56763671e-02  -4.77295145e-02 ...,   9.65146795e-02\n",
      "   -2.91924528e-03  -1.14561766e-02]\n",
      " [ -3.29067893e-02  -5.85077107e-02   8.12792853e-02 ...,  -3.09921913e-02\n",
      "    3.40251587e-02  -1.34395724e-02]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "en_swad_fn = '/home/eszti/data/panlex_swadesh/swadesh110/test/eng-000.txt'\n",
    "en_embed_fn = '/mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec'\n",
    "\n",
    "en_swad, en_emb, en_nfi = get_embedding(en_swad_fn, swad_idx, en_embed_fn)\n",
    "print(en_swad)\n",
    "print(en_emb)\n",
    "print(en_nfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load German embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'all', u'asche', u'borke', u'bauch', u'gross', u'vogel', u'beissen', u'schwarz', u'blut', u'knochen', u'brust', u'brennen', u'kralle', u'wolke', u'kalt', u'kommen', u'sterben', u'hund', u'trinken', u'trocken', u'ohr', u'erde', u'essen', u'ei', u'auge', u'dick', u'feder', u'feuer', u'fisch', u'fliegen', u'fuss', u'voll', u'geben', u'gut', u'gr\\xfcn', u'haar', u'hand', u'kopf', u'h\\xf6ren', u'herz', u'horn', u'ich', u'umbringen', u'knie', u'kennen', u'blatt', u'liegen', u'leber', u'lang', u'laus', u'mann', u'viel', u'fleisch', u'mond', u'berg', u'mund', u'name', u'nacken', u'neu', u'nacht', u'nase', u'nicht', u'ein', u'mensch', u'regen', u'rot', u'pfad', u'wurzel', u'rund', u'sand', u'sagen', u'sehen', u'samen', u'sitzen', u'haut', u'schlafen', u'klein', u'rauch', u'stehen', u'stern', u'stein', u'sonne', u'schwimmen', u'schwanz', u'das', u'dieses', u'du', u'zunge', u'zahn', u'baum', u'zwei', u'gehen', u'warm', u'wasser', u'wir', u'was', u'weiss', u'wer', u'frau', u'gelb', u'weit', u'schwer', u'nah', u'salz', u'kurz', u'schlange', u'd\\xfcnn', u'wind', u'wurm', u'jahr']\n",
      "[[-0.06213669  0.05129817 -0.04741741 ..., -0.02501231 -0.01856263\n",
      "   0.06243059]\n",
      " [-0.03386743  0.04923715 -0.03906514 ..., -0.0276785  -0.02018742\n",
      "   0.06505576]\n",
      " [-0.07601882  0.07272251 -0.04302153 ...,  0.06049242  0.09194499\n",
      "   0.14450699]\n",
      " ..., \n",
      " [-0.10472649  0.04958409  0.03859929 ..., -0.03333807  0.0022068\n",
      "   0.06766976]\n",
      " [-0.00937888  0.07365981  0.0020786  ...,  0.0239687   0.01208459\n",
      "   0.13262165]\n",
      " [-0.09250893 -0.02600697 -0.09789319 ..., -0.05426902  0.03866027\n",
      "   0.11533482]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "de_swad_fn = '/home/eszti/data/panlex_swadesh/swadesh110/test/deu.txt'\n",
    "de_embed_fn = '/mnt/permanent/Language/Multi/FB/wiki.de/wiki.de.vec'\n",
    "\n",
    "de_swad, de_emb, de_nfi = get_embedding(de_swad_fn, swad_idx, de_embed_fn)\n",
    "\n",
    "print(de_swad)\n",
    "print(de_emb)\n",
    "print(de_nfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check what the similar words are in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dog',\n",
       " u'bite',\n",
       " u'louse',\n",
       " u'claw',\n",
       " u'bird',\n",
       " u'snake',\n",
       " u'eat',\n",
       " u'nose',\n",
       " u'hair',\n",
       " u'ear',\n",
       " u'fat',\n",
       " u'tooth',\n",
       " u'bone',\n",
       " u'fish',\n",
       " u'foot',\n",
       " u'tail',\n",
       " u'egg',\n",
       " u'cold',\n",
       " u'skin',\n",
       " u'hot',\n",
       " u'eye',\n",
       " u'feather',\n",
       " u'tongue',\n",
       " u'bark',\n",
       " u'mouth',\n",
       " u'flesh',\n",
       " u'little',\n",
       " u'drink',\n",
       " u'kill',\n",
       " u'mountain',\n",
       " u'stand',\n",
       " u'sleep',\n",
       " u'go',\n",
       " u'night',\n",
       " u'head',\n",
       " u'neck',\n",
       " u'big',\n",
       " u'hand',\n",
       " u'good',\n",
       " u'blood',\n",
       " u'heart',\n",
       " u'horn',\n",
       " u'swim',\n",
       " u'liver',\n",
       " u'human',\n",
       " u'stone',\n",
       " u'worm',\n",
       " u'hear',\n",
       " u'moon',\n",
       " u'say',\n",
       " u'you',\n",
       " u'sit',\n",
       " u'that',\n",
       " u'fire',\n",
       " u'black',\n",
       " u'know',\n",
       " u'tree',\n",
       " u'white',\n",
       " u'what',\n",
       " u'fly',\n",
       " u'come',\n",
       " u'red',\n",
       " u'breast',\n",
       " u'sand',\n",
       " u'we',\n",
       " u'yellow',\n",
       " u'short',\n",
       " u'adulterous',\n",
       " u'smoke',\n",
       " u'dry',\n",
       " u'name',\n",
       " u'male',\n",
       " u'long',\n",
       " u'not',\n",
       " u'heavy',\n",
       " u'knee',\n",
       " u'rain',\n",
       " u'root',\n",
       " u'star',\n",
       " u'thin',\n",
       " u'path',\n",
       " u'far',\n",
       " u'who',\n",
       " u'female',\n",
       " u'abdomen',\n",
       " u'wind',\n",
       " u'leaf',\n",
       " u'many',\n",
       " u'water',\n",
       " u'salt',\n",
       " u'this',\n",
       " u'year',\n",
       " u'die',\n",
       " u'two',\n",
       " u'all',\n",
       " u'lie',\n",
       " u'cloud',\n",
       " u'sun',\n",
       " u'earth',\n",
       " u'as',\n",
       " u'see',\n",
       " u'i',\n",
       " u'burn',\n",
       " u'give',\n",
       " u'near',\n",
       " u'round',\n",
       " u'seed',\n",
       " u'green',\n",
       " u'new',\n",
       " u'full']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, sims_en = get_corr(en_emb, en_swad)\n",
    "sims_en['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check what the similar words are in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hund',\n",
       " u'mensch',\n",
       " u'schwanz',\n",
       " u'mann',\n",
       " u'beissen',\n",
       " u'schlange',\n",
       " u'frau',\n",
       " u'nase',\n",
       " u'vogel',\n",
       " u'schlafen',\n",
       " u'ohr',\n",
       " u'bauch',\n",
       " u'kralle',\n",
       " u'fisch',\n",
       " u'fleisch',\n",
       " u'knochen',\n",
       " u'kopf',\n",
       " u'haut',\n",
       " u'trinken',\n",
       " u'mund',\n",
       " u'haar',\n",
       " u'umbringen',\n",
       " u'zunge',\n",
       " u'leber',\n",
       " u'blut',\n",
       " u'nacken',\n",
       " u'brust',\n",
       " u'mond',\n",
       " u'stein',\n",
       " u'baum',\n",
       " u'wer',\n",
       " u'laus',\n",
       " u'sagen',\n",
       " u'sterben',\n",
       " u'wolke',\n",
       " u'knie',\n",
       " u'herz',\n",
       " u'gut',\n",
       " u'hand',\n",
       " u'auge',\n",
       " u'wurm',\n",
       " u'h\\xf6ren',\n",
       " u'was',\n",
       " u'fliegen',\n",
       " u'kennen',\n",
       " u'nicht',\n",
       " u'wir',\n",
       " u'viel',\n",
       " u'fuss',\n",
       " u'stern',\n",
       " u'nacht',\n",
       " u'ei',\n",
       " u'zahn',\n",
       " u'rauch',\n",
       " u'sehen',\n",
       " u'dick',\n",
       " u'ich',\n",
       " u'ein',\n",
       " u'weiss',\n",
       " u'essen',\n",
       " u'feder',\n",
       " u'du',\n",
       " u'schwer',\n",
       " u'wurzel',\n",
       " u'wasser',\n",
       " u'erde',\n",
       " u'schwimmen',\n",
       " u'sonne',\n",
       " u'borke',\n",
       " u'feuer',\n",
       " u'sitzen',\n",
       " u'pfad',\n",
       " u'wind',\n",
       " u'gehen',\n",
       " u'kalt',\n",
       " u'voll',\n",
       " u'schwarz',\n",
       " u'berg',\n",
       " u'regen',\n",
       " u'gross',\n",
       " u'brennen',\n",
       " u'klein',\n",
       " u'geben',\n",
       " u'warm',\n",
       " u'das',\n",
       " u'horn',\n",
       " u'nah',\n",
       " u'salz',\n",
       " u'name',\n",
       " u'lang',\n",
       " u'zwei',\n",
       " u'asche',\n",
       " u'jahr',\n",
       " u'samen',\n",
       " u'rot',\n",
       " u'sand',\n",
       " u'trocken',\n",
       " u'blatt',\n",
       " u'kurz',\n",
       " u'stehen',\n",
       " u'kommen',\n",
       " u'rund',\n",
       " u'dieses',\n",
       " u'all',\n",
       " u'gr\\xfcn',\n",
       " u'd\\xfcnn',\n",
       " u'weit',\n",
       " u'gelb',\n",
       " u'liegen',\n",
       " u'neu']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, sims_de = get_corr(de_emb, de_swad)\n",
    "sims_de['hund']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train universal embedding based on English and German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 386.768829\n",
      "Loss at step 100: 379.394958\n",
      "Loss at step 200: 372.529663\n",
      "Loss at step 300: 366.101318\n",
      "Loss at step 400: 360.043884\n",
      "Loss at step 500: 354.297607\n",
      "Loss at step 600: 348.810577\n",
      "Loss at step 700: 343.538330\n",
      "Loss at step 800: 338.442688\n",
      "Loss at step 900: 333.491821\n",
      "Loss at step 1000: 328.661011\n",
      "Loss at step 1100: 323.927979\n",
      "Loss at step 1200: 319.276276\n",
      "Loss at step 1300: 314.692871\n",
      "Loss at step 1400: 310.166809\n",
      "Loss at step 1500: 305.689850\n",
      "Loss at step 1600: 301.255463\n",
      "Loss at step 1700: 296.859192\n",
      "Loss at step 1800: 292.497009\n",
      "Loss at step 1900: 288.166016\n",
      "Loss at step 2000: 283.864563\n",
      "Loss at step 2100: 279.591309\n",
      "Loss at step 2200: 275.345062\n",
      "Loss at step 2300: 271.125122\n",
      "Loss at step 2400: 266.931458\n",
      "Loss at step 2500: 262.763947\n",
      "Loss at step 2600: 258.622375\n",
      "Loss at step 2700: 254.507156\n",
      "Loss at step 2800: 250.418518\n",
      "Loss at step 2900: 246.356934\n",
      "Loss at step 3000: 242.322845\n",
      "Loss at step 3100: 238.316513\n",
      "Loss at step 3200: 234.338730\n",
      "Loss at step 3300: 230.390228\n",
      "Loss at step 3400: 226.471283\n",
      "Loss at step 3500: 222.582733\n",
      "Loss at step 3600: 218.725296\n",
      "Loss at step 3700: 214.899811\n",
      "Loss at step 3800: 211.106796\n",
      "Loss at step 3900: 207.347122\n",
      "Loss at step 4000: 203.621704\n",
      "Loss at step 4100: 199.931366\n",
      "Loss at step 4200: 196.276947\n",
      "Loss at step 4300: 192.659302\n",
      "Loss at step 4400: 189.079468\n",
      "Loss at step 4500: 185.538391\n",
      "Loss at step 4600: 182.036926\n",
      "Loss at step 4700: 178.576324\n",
      "Loss at step 4800: 175.157349\n",
      "Loss at step 4900: 171.781219\n",
      "Loss at step 5000: 168.449188\n",
      "Loss at step 5100: 165.162048\n",
      "Loss at step 5200: 161.920944\n",
      "Loss at step 5300: 158.727203\n",
      "Loss at step 5400: 155.581741\n",
      "Loss at step 5500: 152.485931\n",
      "Loss at step 5600: 149.440765\n",
      "Loss at step 5700: 146.447342\n",
      "Loss at step 5800: 143.506912\n",
      "Loss at step 5900: 140.620483\n",
      "Loss at step 6000: 137.789215\n",
      "Loss at step 6100: 135.014008\n",
      "Loss at step 6200: 132.296036\n",
      "Loss at step 6300: 129.636063\n",
      "Loss at step 6400: 127.035217\n",
      "Loss at step 6500: 124.494110\n",
      "Loss at step 6600: 122.013657\n",
      "Loss at step 6700: 119.594330\n",
      "Loss at step 6800: 117.236740\n",
      "Loss at step 6900: 114.941429\n",
      "Loss at step 7000: 112.708679\n",
      "Loss at step 7100: 110.538689\n",
      "Loss at step 7200: 108.431396\n",
      "Loss at step 7300: 106.386879\n",
      "Loss at step 7400: 104.404938\n",
      "Loss at step 7500: 102.485161\n",
      "Loss at step 7600: 100.627029\n",
      "Loss at step 7700: 98.829887\n",
      "Loss at step 7800: 97.092957\n",
      "Loss at step 7900: 95.415268\n",
      "Loss at step 8000: 93.795639\n",
      "Loss at step 8100: 92.233093\n",
      "Loss at step 8200: 90.725960\n",
      "Loss at step 8300: 89.272919\n",
      "Loss at step 8400: 87.872467\n",
      "Loss at step 8500: 86.522827\n",
      "Loss at step 8600: 85.222313\n",
      "Loss at step 8700: 83.969055\n",
      "Loss at step 8800: 82.761269\n",
      "Loss at step 8900: 81.597038\n",
      "Loss at step 9000: 80.474442\n",
      "Loss at step 9100: 79.391586\n",
      "Loss at step 9200: 78.346420\n",
      "Loss at step 9300: 77.337212\n",
      "Loss at step 9400: 76.361961\n",
      "Loss at step 9500: 75.418854\n",
      "Loss at step 9600: 74.506172\n",
      "Loss at step 9700: 73.621986\n",
      "Loss at step 9800: 72.764786\n",
      "Loss at step 9900: 71.932907\n",
      "Loss at step 10000: 71.124802\n",
      "Loss at step 10100: 70.338905\n",
      "Loss at step 10200: 69.573891\n",
      "Loss at step 10300: 68.828476\n",
      "Loss at step 10400: 68.101166\n",
      "Loss at step 10500: 67.391006\n",
      "Loss at step 10600: 66.696907\n",
      "Loss at step 10700: 66.017670\n",
      "Loss at step 10800: 65.352448\n",
      "Loss at step 10900: 64.700272\n",
      "Loss at step 11000: 64.060410\n",
      "Loss at step 11100: 63.431969\n",
      "Loss at step 11200: 62.814350\n",
      "Loss at step 11300: 62.206837\n",
      "Loss at step 11400: 61.608829\n",
      "Loss at step 11500: 61.019817\n",
      "Loss at step 11600: 60.439232\n",
      "Loss at step 11700: 59.866699\n",
      "Loss at step 11800: 59.301758\n",
      "Loss at step 11900: 58.744019\n",
      "Loss at step 12000: 58.193180\n",
      "Loss at step 12100: 57.648834\n",
      "Loss at step 12200: 57.110828\n",
      "Loss at step 12300: 56.578796\n",
      "Loss at step 12400: 56.052559\n",
      "Loss at step 12500: 55.531910\n",
      "Loss at step 12600: 55.016655\n",
      "Loss at step 12700: 54.506599\n",
      "Loss at step 12800: 54.001648\n",
      "Loss at step 12900: 53.501587\n",
      "Loss at step 13000: 53.006344\n",
      "Loss at step 13100: 52.515789\n",
      "Loss at step 13200: 52.029827\n",
      "Loss at step 13300: 51.548355\n",
      "Loss at step 13400: 51.071293\n",
      "Loss at step 13500: 50.598511\n",
      "Loss at step 13600: 50.130054\n",
      "Loss at step 13700: 49.665760\n",
      "Loss at step 13800: 49.205620\n",
      "Loss at step 13900: 48.749577\n",
      "Loss at step 14000: 48.297493\n",
      "Loss at step 14100: 47.849422\n",
      "Loss at step 14200: 47.405270\n",
      "Loss at step 14300: 46.964996\n",
      "Loss at step 14400: 46.528542\n",
      "Loss at step 14500: 46.095882\n",
      "Loss at step 14600: 45.776871\n",
      "Loss at step 14700: 45.463211\n",
      "Loss at step 14800: 45.150867\n",
      "Loss at step 14900: 44.839687\n",
      "Loss at step 15000: 44.529663\n",
      "Loss at step 15100: 44.220856\n",
      "Loss at step 15200: 43.913155\n",
      "Loss at step 15300: 43.606728\n",
      "Loss at step 15400: 43.301441\n",
      "Loss at step 15500: 42.997299\n",
      "Loss at step 15600: 42.694408\n",
      "Loss at step 15700: 42.392685\n",
      "Loss at step 15800: 42.092159\n",
      "Loss at step 15900: 41.792797\n",
      "Loss at step 16000: 41.494644\n",
      "Loss at step 16100: 41.197697\n",
      "Loss at step 16200: 40.901939\n",
      "Loss at step 16300: 40.607384\n",
      "Loss at step 16400: 40.314034\n",
      "Loss at step 16500: 40.021904\n",
      "Loss at step 16600: 39.730980\n",
      "Loss at step 16700: 39.441254\n",
      "Loss at step 16800: 39.152702\n",
      "Loss at step 16900: 38.865410\n",
      "Loss at step 17000: 38.579323\n",
      "Loss at step 17100: 38.294445\n",
      "Loss at step 17200: 38.010796\n",
      "Loss at step 17300: 37.728329\n",
      "Loss at step 17400: 37.447121\n",
      "Loss at step 17500: 37.167099\n",
      "Loss at step 17600: 36.888355\n",
      "Loss at step 17700: 36.610764\n",
      "Loss at step 17800: 36.334415\n",
      "Loss at step 17900: 36.059315\n",
      "Loss at step 18000: 35.785442\n",
      "Loss at step 18100: 35.512760\n",
      "Loss at step 18200: 35.241322\n",
      "Loss at step 18300: 34.971123\n",
      "Loss at step 18400: 34.702141\n",
      "Loss at step 18500: 34.434383\n",
      "Loss at step 18600: 34.167862\n",
      "Loss at step 18700: 33.902569\n",
      "Loss at step 18800: 33.638523\n",
      "Loss at step 18900: 33.375694\n",
      "Loss at step 19000: 33.114094\n",
      "Loss at step 19100: 32.853733\n",
      "Loss at step 19200: 32.594604\n",
      "Loss at step 19300: 32.336723\n",
      "Loss at step 19400: 32.080036\n",
      "Loss at step 19500: 31.824606\n",
      "Loss at step 19600: 31.570402\n",
      "Loss at step 19700: 31.317436\n",
      "Loss at step 19800: 31.065689\n",
      "Loss at step 19900: 30.815165\n",
      "Loss at step 20000: 30.565899\n",
      "Loss at step 20100: 30.317827\n",
      "Loss at step 20200: 30.071047\n",
      "Loss at step 20300: 29.825472\n",
      "Loss at step 20400: 29.581116\n",
      "Loss at step 20500: 29.337978\n",
      "Loss at step 20600: 29.096073\n",
      "Loss at step 20700: 28.855394\n",
      "Loss at step 20800: 28.615957\n",
      "Loss at step 20900: 28.377731\n",
      "Loss at step 21000: 28.140720\n",
      "Loss at step 21100: 27.904951\n",
      "Loss at step 21200: 27.670374\n",
      "Loss at step 21300: 27.437033\n",
      "Loss at step 21400: 27.204931\n",
      "Loss at step 21500: 26.974012\n",
      "Loss at step 21600: 26.744322\n",
      "Loss at step 21700: 26.515848\n",
      "Loss at step 21800: 26.288576\n",
      "Loss at step 21900: 26.062515\n",
      "Loss at step 22000: 25.837681\n",
      "Loss at step 22100: 25.614014\n",
      "Loss at step 22200: 25.391573\n",
      "Loss at step 22300: 25.170324\n",
      "Loss at step 22400: 24.950315\n",
      "Loss at step 22500: 24.731461\n",
      "Loss at step 22600: 24.513807\n",
      "Loss at step 22700: 24.297348\n",
      "Loss at step 22800: 24.082071\n",
      "Loss at step 22900: 23.867973\n",
      "Loss at step 23000: 23.655087\n",
      "Loss at step 23100: 23.443359\n",
      "Loss at step 23200: 23.232828\n",
      "Loss at step 23300: 23.023438\n",
      "Loss at step 23400: 22.815239\n",
      "Loss at step 23500: 22.608200\n",
      "Loss at step 23600: 22.402321\n",
      "Loss at step 23700: 22.197641\n",
      "Loss at step 23800: 21.994076\n",
      "Loss at step 23900: 21.791691\n",
      "Loss at step 24000: 21.590446\n",
      "Loss at step 24100: 21.390354\n",
      "Loss at step 24200: 21.191410\n",
      "Loss at step 24300: 20.993595\n",
      "Loss at step 24400: 20.796904\n",
      "Loss at step 24500: 20.601372\n",
      "Loss at step 24600: 20.406939\n",
      "Loss at step 24700: 20.213665\n",
      "Loss at step 24800: 20.021492\n",
      "Loss at step 24900: 19.830444\n",
      "Loss at step 25000: 19.640490\n",
      "Loss at step 25100: 19.451656\n",
      "Loss at step 25200: 19.263914\n",
      "Loss at step 25300: 19.077271\n",
      "Loss at step 25400: 18.891724\n",
      "Loss at step 25500: 18.707289\n",
      "Loss at step 25600: 18.523916\n",
      "Loss at step 25700: 18.341612\n",
      "Loss at step 25800: 18.160387\n",
      "Loss at step 25900: 17.980238\n",
      "Loss at step 26000: 17.801157\n",
      "Loss at step 26100: 17.623133\n",
      "Loss at step 26200: 17.446173\n",
      "Loss at step 26300: 17.270247\n",
      "Loss at step 26400: 17.095392\n",
      "Loss at step 26500: 16.921551\n",
      "Loss at step 26600: 16.748751\n",
      "Loss at step 26700: 16.576988\n",
      "Loss at step 26800: 16.406235\n",
      "Loss at step 26900: 16.236507\n",
      "Loss at step 27000: 16.067808\n",
      "Loss at step 27100: 15.900104\n",
      "Loss at step 27200: 15.733397\n",
      "Loss at step 27300: 15.567700\n",
      "Loss at step 27400: 15.402987\n",
      "Loss at step 27500: 15.239270\n",
      "Loss at step 27600: 15.076525\n",
      "Loss at step 27700: 14.914757\n",
      "Loss at step 27800: 14.753951\n",
      "Loss at step 27900: 14.594120\n",
      "Loss at step 28000: 14.435250\n",
      "Loss at step 28100: 14.277330\n",
      "Loss at step 28200: 14.120360\n",
      "Loss at step 28300: 13.964327\n",
      "Loss at step 28400: 13.809241\n",
      "Loss at step 28500: 13.655074\n",
      "Loss at step 28600: 13.501833\n",
      "Loss at step 28700: 13.349511\n",
      "Loss at step 28800: 13.198100\n",
      "Loss at step 28900: 13.047619\n",
      "Loss at step 29000: 12.898016\n",
      "Loss at step 29100: 12.749318\n",
      "Loss at step 29200: 12.601502\n",
      "Loss at step 29300: 12.454583\n",
      "Loss at step 29400: 12.308537\n",
      "Loss at step 29500: 12.163364\n",
      "Loss at step 29600: 12.019075\n",
      "Loss at step 29700: 11.875634\n",
      "Loss at step 29800: 11.733055\n",
      "Loss at step 29900: 11.591325\n",
      "Loss at step 30000: 11.450444\n",
      "Loss at step 30100: 11.310392\n",
      "Loss at step 30200: 11.171194\n",
      "Loss at step 30300: 11.032816\n",
      "Loss at step 30400: 10.895257\n",
      "Loss at step 30500: 10.758522\n",
      "Loss at step 30600: 10.622602\n",
      "Loss at step 30700: 10.487484\n",
      "Loss at step 30800: 10.353174\n",
      "Loss at step 30900: 10.219655\n",
      "Loss at step 31000: 10.086932\n",
      "Loss at step 31100: 9.954999\n",
      "Loss at step 31200: 9.823845\n",
      "Loss at step 31300: 9.693467\n",
      "Loss at step 31400: 9.563856\n",
      "Loss at step 31500: 9.435014\n",
      "Loss at step 31600: 9.306936\n",
      "Loss at step 31700: 9.179605\n",
      "Loss at step 31800: 9.053035\n",
      "Loss at step 31900: 8.927206\n",
      "Loss at step 32000: 8.802115\n",
      "Loss at step 32100: 8.677755\n",
      "Loss at step 32200: 8.554144\n",
      "Loss at step 32300: 8.431250\n",
      "Loss at step 32400: 8.309083\n",
      "Loss at step 32500: 8.187625\n",
      "Loss at step 32600: 8.066877\n",
      "Loss at step 32700: 7.946842\n",
      "Loss at step 32800: 7.827520\n",
      "Loss at step 32900: 7.708892\n",
      "Loss at step 33000: 7.590952\n",
      "Loss at step 33100: 7.473708\n",
      "Loss at step 33200: 7.357149\n",
      "Loss at step 33300: 7.241279\n",
      "Loss at step 33400: 7.126077\n",
      "Loss at step 33500: 7.011555\n",
      "Loss at step 33600: 6.897699\n",
      "Loss at step 33700: 6.784521\n",
      "Loss at step 33800: 6.671994\n",
      "Loss at step 33900: 6.560133\n",
      "Loss at step 34000: 6.448916\n",
      "Loss at step 34100: 6.338364\n",
      "Loss at step 34200: 6.228457\n",
      "Loss at step 34300: 6.119189\n",
      "Loss at step 34400: 6.010560\n",
      "Loss at step 34500: 5.902570\n",
      "Loss at step 34600: 5.795218\n",
      "Loss at step 34700: 5.688494\n",
      "Loss at step 34800: 5.582397\n",
      "Loss at step 34900: 5.476923\n",
      "Loss at step 35000: 5.372070\n",
      "Loss at step 35100: 5.267840\n",
      "Loss at step 35200: 5.164221\n",
      "Loss at step 35300: 5.061216\n",
      "Loss at step 35400: 4.958824\n",
      "Loss at step 35500: 4.857037\n",
      "Loss at step 35600: 4.755855\n",
      "Loss at step 35700: 4.655274\n",
      "Loss at step 35800: 4.555294\n",
      "Loss at step 35900: 4.455912\n",
      "Loss at step 36000: 4.357122\n",
      "Loss at step 36100: 4.258928\n",
      "Loss at step 36200: 4.161327\n",
      "Loss at step 36300: 4.064311\n",
      "Loss at step 36400: 3.967888\n",
      "Loss at step 36500: 3.872046\n",
      "Loss at step 36600: 3.776788\n",
      "Loss at step 36700: 3.682114\n",
      "Loss at step 36800: 3.588020\n",
      "Loss at step 36900: 3.494506\n",
      "Loss at step 37000: 3.401573\n",
      "Loss at step 37100: 3.309212\n",
      "Loss at step 37200: 3.217428\n",
      "Loss at step 37300: 3.126226\n",
      "Loss at step 37400: 3.035591\n",
      "Loss at step 37500: 2.945536\n",
      "Loss at step 37600: 2.856051\n",
      "Loss at step 37700: 2.767138\n",
      "Loss at step 37800: 2.678802\n",
      "Loss at step 37900: 2.591040\n",
      "Loss at step 38000: 2.503849\n",
      "Loss at step 38100: 2.417232\n",
      "Loss at step 38200: 2.331191\n",
      "Loss at step 38300: 2.245726\n",
      "Loss at step 38400: 2.160838\n",
      "Loss at step 38500: 2.076529\n",
      "Loss at step 38600: 1.992800\n",
      "Loss at step 38700: 1.909650\n",
      "Loss at step 38800: 1.827088\n",
      "Loss at step 38900: 1.745109\n",
      "Loss at step 39000: 1.663722\n",
      "Loss at step 39100: 1.582927\n",
      "Loss at step 39200: 1.502730\n",
      "Loss at step 39300: 1.423133\n",
      "Loss at step 39400: 1.344143\n",
      "Loss at step 39500: 1.265763\n",
      "Loss at step 39600: 1.188002\n",
      "Loss at step 39700: 1.110866\n",
      "Loss at step 39800: 1.034362\n",
      "Loss at step 39900: 0.958499\n",
      "Loss at step 40000: 0.883288\n",
      "Loss at step 40100: 0.808740\n",
      "Loss at step 40200: 0.734868\n",
      "Loss at step 40300: 0.661688\n",
      "Loss at step 40400: 0.589218\n",
      "Loss at step 40500: 0.517479\n",
      "Loss at step 40600: 0.446498\n",
      "Loss at step 40700: 0.376307\n",
      "Loss at step 40800: 0.306952\n",
      "Loss at step 40900: 0.238511\n",
      "Loss at step 41000: 0.171399\n",
      "Loss at step 41100: 0.151650\n",
      "Loss at step 41200: 0.151869\n",
      "Loss at step 41300: 0.151920\n",
      "Loss at step 41400: 0.151936\n",
      "Loss at step 41500: 0.151942\n",
      "Loss at step 41600: 0.151945\n",
      "Loss at step 41700: 0.151944\n",
      "Loss at step 41800: 0.151945\n",
      "Loss at step 41900: 0.151950\n",
      "Loss at step 42000: 0.151948\n",
      "Loss at step 42100: 0.151948\n",
      "Loss at step 42200: 0.151947\n",
      "Loss at step 42300: 0.151944\n",
      "Loss at step 42400: 0.151941\n",
      "Loss at step 42500: 0.151938\n",
      "Loss at step 42600: 0.151935\n",
      "Loss at step 42700: 0.151935\n",
      "Loss at step 42800: 0.151935\n",
      "Loss at step 42900: 0.151932\n",
      "Loss at step 43000: 0.151933\n",
      "Loss at step 43100: 0.151935\n",
      "Loss at step 43200: 0.151935\n",
      "Loss at step 43300: 0.151932\n",
      "Loss at step 43400: 0.151932\n",
      "Loss at step 43500: 0.151930\n",
      "Loss at step 43600: 0.151928\n",
      "Loss at step 43700: 0.151930\n",
      "Loss at step 43800: 0.151931\n",
      "Loss at step 43900: 0.151932\n",
      "Loss at step 44000: 0.151930\n",
      "Loss at step 44100: 0.151928\n",
      "Loss at step 44200: 0.151927\n",
      "Loss at step 44300: 0.151925\n",
      "Loss at step 44400: 0.151922\n",
      "Loss at step 44500: 0.151920\n",
      "Loss at step 44600: 0.151919\n",
      "Loss at step 44700: 0.151920\n",
      "Loss at step 44800: 0.151923\n",
      "Loss at step 44900: 0.151923\n",
      "Loss at step 45000: 0.151924\n",
      "Loss at step 45100: 0.151922\n",
      "Loss at step 45200: 0.151924\n",
      "Loss at step 45300: 0.151922\n",
      "Loss at step 45400: 0.151922\n",
      "Loss at step 45500: 0.151919\n",
      "Loss at step 45600: 0.151918\n",
      "Loss at step 45700: 0.151917\n",
      "Loss at step 45800: 0.151919\n",
      "Loss at step 45900: 0.151918\n",
      "Loss at step 46000: 0.151917\n",
      "Loss at step 46100: 0.151917\n",
      "Loss at step 46200: 0.151914\n",
      "Loss at step 46300: 0.151913\n",
      "Loss at step 46400: 0.151911\n",
      "Loss at step 46500: 0.151911\n",
      "Loss at step 46600: 0.151911\n",
      "Loss at step 46700: 0.151910\n",
      "Loss at step 46800: 0.151912\n",
      "Loss at step 46900: 0.151913\n",
      "Loss at step 47000: 0.151912\n",
      "Loss at step 47100: 0.151913\n",
      "Loss at step 47200: 0.151915\n",
      "Loss at step 47300: 0.151916\n",
      "Loss at step 47400: 0.151916\n",
      "Loss at step 47500: 0.151914\n",
      "Loss at step 47600: 0.151913\n",
      "Loss at step 47700: 0.151913\n",
      "Loss at step 47800: 0.151914\n",
      "Loss at step 47900: 0.151914\n",
      "Loss at step 48000: 0.151913\n",
      "Loss at step 48100: 0.151917\n",
      "Loss at step 48200: 0.151914\n",
      "Loss at step 48300: 0.151914\n",
      "Loss at step 48400: 0.151914\n",
      "Loss at step 48500: 0.151917\n",
      "Loss at step 48600: 0.151920\n",
      "Loss at step 48700: 0.151923\n",
      "Loss at step 48800: 0.151922\n",
      "Loss at step 48900: 0.151921\n",
      "Loss at step 49000: 0.151921\n",
      "Loss at step 49100: 0.151922\n",
      "Loss at step 49200: 0.151921\n",
      "Loss at step 49300: 0.151920\n",
      "Loss at step 49400: 0.151919\n",
      "Loss at step 49500: 0.151919\n",
      "Loss at step 49600: 0.151916\n",
      "Loss at step 49700: 0.151915\n",
      "Loss at step 49800: 0.151916\n",
      "Loss at step 49900: 0.151916\n",
      "\n",
      "\n",
      "Transform 1:\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Transform 2:\n",
      "[[ 1.54746592 -0.43078005 -0.41038191 ...,  0.01547258  0.22070003\n",
      "   0.13435145]\n",
      " [-1.27935648 -0.73427206  0.84574866 ..., -0.34298468 -1.17742717\n",
      "   0.79275841]\n",
      " [ 0.42788631 -0.63659704 -0.19598079 ...,  1.00769746 -0.94639814\n",
      "  -0.04775887]\n",
      " ..., \n",
      " [ 0.38290027  0.6357199  -0.35400206 ...,  1.09509194 -0.6037547\n",
      "  -0.31756675]\n",
      " [ 1.04109812 -1.08863068  0.56958312 ...,  0.03990827 -0.41337264\n",
      "   0.55595607]\n",
      " [ 0.47588512 -2.03216624  0.25621423 ..., -0.85773802 -0.02164155\n",
      "   1.31181347]]\n",
      "Universal embedding:\n",
      "[[ -1.62685037e-01   3.34574329e-03  -7.70276710e-02 ...,  -6.63763955e-02\n",
      "    4.98947911e-02  -1.09160664e-02]\n",
      " [ -4.37652916e-02  -1.49745783e-02  -4.36131693e-02 ...,  -3.50847887e-03\n",
      "    3.43946740e-05   5.55294938e-02]\n",
      " [ -8.16370845e-02  -2.41310131e-02  -3.51970941e-02 ...,   1.93154458e-02\n",
      "    4.08360846e-02   5.56615926e-02]\n",
      " ..., \n",
      " [ -1.28755182e-01  -6.26038481e-03  -2.87935901e-02 ...,   3.85492966e-02\n",
      "    1.30377516e-01   2.86402293e-02]\n",
      " [ -4.98850569e-02  -5.56938685e-02  -4.77996729e-02 ...,   9.65856314e-02\n",
      "   -2.87217367e-03  -1.15052387e-02]\n",
      " [ -3.29606459e-02  -5.85216880e-02   8.12231824e-02 ...,  -3.09354458e-02\n",
      "    3.40627916e-02  -1.34788109e-02]]\n",
      "\n",
      "\n",
      "W1*T1:\n",
      "[[ -1.62625358e-01   3.36121395e-03  -7.69655257e-02 ...,  -6.64392561e-02\n",
      "    4.98530976e-02  -1.08726015e-02]\n",
      " [ -4.36824225e-02  -1.49530806e-02  -4.35268208e-02 ...,  -3.59577429e-03\n",
      "   -2.35032567e-05   5.55898510e-02]\n",
      " [ -8.15486684e-02  -2.41080951e-02  -3.51050124e-02 ...,   1.92223284e-02\n",
      "    4.07743342e-02   5.57259880e-02]\n",
      " ..., \n",
      " [ -1.28676072e-01  -6.23986265e-03  -2.87111904e-02 ...,   3.84659730e-02\n",
      "    1.30322233e-01   2.86978595e-02]\n",
      " [ -4.98176925e-02  -5.56763671e-02  -4.77295145e-02 ...,   9.65146795e-02\n",
      "   -2.91924528e-03  -1.14561766e-02]\n",
      " [ -3.29067893e-02  -5.85077107e-02   8.12792853e-02 ...,  -3.09921913e-02\n",
      "    3.40251587e-02  -1.34395724e-02]]\n",
      "W2*T2:\n",
      "[[-0.16183805  0.00356518 -0.0761457  ..., -0.06726848  0.04930302\n",
      "  -0.01029925]\n",
      " [-0.04258914 -0.01466951 -0.04238757 ..., -0.00474739 -0.00078741\n",
      "   0.05638619]\n",
      " [-0.08038212 -0.02380556 -0.03389019 ...,  0.01799385  0.03995946\n",
      "   0.05657548]\n",
      " ..., \n",
      " [-0.12763242 -0.00596932 -0.02762416 ...,  0.03736665  0.12959293\n",
      "   0.02945819]\n",
      " [-0.04892892 -0.05544528 -0.04680411 ...,  0.09557861 -0.00354022\n",
      "  -0.01080894]\n",
      " [-0.03219625 -0.05832329  0.08201952 ..., -0.03174069  0.03352869\n",
      "  -0.01292202]]\n"
     ]
    }
   ],
   "source": [
    "    W = np.ndarray(shape=(2, len(en_swad), en_emb.shape[1]), dtype=np.float32)\n",
    "    W[0, :, :] = en_emb\n",
    "    W[1, :, :] = de_emb\n",
    "    T1, T, A = train(W, num_steps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corr_mx, sim_corr, sims_univ = get_corr(A, en_swad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000024,  0.27097768,  0.14398828, ...,  0.22070432,\n",
       "         0.17658763,  0.25892368],\n",
       "       [ 0.27097768,  0.99999994,  0.13445655, ...,  0.11244301,\n",
       "         0.15854077,  0.25677812],\n",
       "       [ 0.14398828,  0.13445655,  1.00000012, ...,  0.27488026,\n",
       "         0.28635505,  0.09526135],\n",
       "       ..., \n",
       "       [ 0.22070432,  0.11244301,  0.27488026, ...,  0.99999994,\n",
       "         0.23121968,  0.15674552],\n",
       "       [ 0.17658763,  0.15854077,  0.28635505, ...,  0.23121968,\n",
       "         1.00000012,  0.12600701],\n",
       "       [ 0.25892368,  0.25677812,  0.09526135, ...,  0.15674552,\n",
       "         0.12600701,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check what the similar words are in the universal embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'dog',\n",
       " u'bite',\n",
       " u'louse',\n",
       " u'claw',\n",
       " u'bird',\n",
       " u'snake',\n",
       " u'eat',\n",
       " u'nose',\n",
       " u'hair',\n",
       " u'ear',\n",
       " u'fat',\n",
       " u'tooth',\n",
       " u'bone',\n",
       " u'fish',\n",
       " u'foot',\n",
       " u'tail',\n",
       " u'egg',\n",
       " u'cold',\n",
       " u'skin',\n",
       " u'hot',\n",
       " u'eye',\n",
       " u'feather',\n",
       " u'tongue',\n",
       " u'bark',\n",
       " u'mouth',\n",
       " u'flesh',\n",
       " u'little',\n",
       " u'drink',\n",
       " u'kill',\n",
       " u'mountain',\n",
       " u'stand',\n",
       " u'sleep',\n",
       " u'go',\n",
       " u'night',\n",
       " u'head',\n",
       " u'neck',\n",
       " u'big',\n",
       " u'hand',\n",
       " u'good',\n",
       " u'blood',\n",
       " u'horn',\n",
       " u'heart',\n",
       " u'swim',\n",
       " u'liver',\n",
       " u'human',\n",
       " u'stone',\n",
       " u'worm',\n",
       " u'hear',\n",
       " u'moon',\n",
       " u'say',\n",
       " u'you',\n",
       " u'sit',\n",
       " u'that',\n",
       " u'fire',\n",
       " u'black',\n",
       " u'know',\n",
       " u'tree',\n",
       " u'white',\n",
       " u'what',\n",
       " u'fly',\n",
       " u'come',\n",
       " u'red',\n",
       " u'breast',\n",
       " u'sand',\n",
       " u'we',\n",
       " u'yellow',\n",
       " u'short',\n",
       " u'adulterous',\n",
       " u'smoke',\n",
       " u'dry',\n",
       " u'name',\n",
       " u'male',\n",
       " u'long',\n",
       " u'not',\n",
       " u'heavy',\n",
       " u'knee',\n",
       " u'rain',\n",
       " u'root',\n",
       " u'star',\n",
       " u'thin',\n",
       " u'path',\n",
       " u'far',\n",
       " u'who',\n",
       " u'female',\n",
       " u'abdomen',\n",
       " u'wind',\n",
       " u'leaf',\n",
       " u'many',\n",
       " u'water',\n",
       " u'salt',\n",
       " u'this',\n",
       " u'year',\n",
       " u'die',\n",
       " u'two',\n",
       " u'all',\n",
       " u'lie',\n",
       " u'cloud',\n",
       " u'sun',\n",
       " u'earth',\n",
       " u'as',\n",
       " u'see',\n",
       " u'i',\n",
       " u'burn',\n",
       " u'give',\n",
       " u'near',\n",
       " u'round',\n",
       " u'seed',\n",
       " u'green',\n",
       " u'new',\n",
       " u'full']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_univ['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.999773\n",
      "0.000689033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06213669,  0.05129817, -0.04741741, ..., -0.02501231,\n",
       "        -0.01856263,  0.06243059],\n",
       "       [-0.03386743,  0.04923715, -0.03906514, ..., -0.0276785 ,\n",
       "        -0.02018742,  0.06505576],\n",
       "       [-0.07601882,  0.07272251, -0.04302153, ...,  0.06049242,\n",
       "         0.09194499,  0.14450699],\n",
       "       ..., \n",
       "       [-0.10472649,  0.04958409,  0.03859929, ..., -0.03333807,\n",
       "         0.0022068 ,  0.06766976],\n",
       "       [-0.00937888,  0.07365981,  0.0020786 , ...,  0.0239687 ,\n",
       "         0.01208459,  0.13262165],\n",
       "       [-0.09250893, -0.02600697, -0.09789319, ..., -0.05426902,\n",
       "         0.03866027,  0.11533482]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "de_emb_trans = np.dot(de_emb, T[0])\n",
    "\n",
    "dog = en_emb[0]\n",
    "hund = de_emb_trans[0]\n",
    "\n",
    "print(np.linalg.norm(dog))\n",
    "print(np.linalg.norm(hund))\n",
    "sim = cosine_similarity(dog.reshape(1, -1), hund.reshape(1, -1))\n",
    "# print(sim)\n",
    "# print(dog)\n",
    "# print(A[0])\n",
    "\n",
    "print(np.linalg.norm(dog - A[0]))\n",
    "de_emb.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
