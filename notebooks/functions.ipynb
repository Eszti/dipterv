{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(W, learning_rate=0.01, num_steps=1001, t1_identity=True):\n",
    "    num_of_langs = W.shape[0]\n",
    "    num_of_words = W[0].shape[0]\n",
    "    dim_of_emb = W[0].shape[1]\n",
    "\n",
    "    # Init graphs\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data.\n",
    "      tf_W = tf.constant(W)\n",
    "      if t1_identity:\n",
    "          tf_T1 = tf.constant(np.identity(dim_of_emb).astype(np.float32))    # T1 = identity\n",
    "\n",
    "      # Variables.\n",
    "      if not t1_identity:\n",
    "        tf_T1 = tf.Variable(tf.truncated_normal([dim_of_emb, dim_of_emb]))\n",
    "      tf_T = tf.Variable(tf.truncated_normal([num_of_langs-1, dim_of_emb, dim_of_emb])) \n",
    "      tf_A = tf.Variable(tf.truncated_normal([num_of_words, dim_of_emb]))\n",
    "\n",
    "      # Training computation\n",
    "      loss = tf.norm(tf.matmul(tf_W[0], tf_T1) - tf_A)                   # T1 may be constant\n",
    "      for i in range(1, num_of_langs):\n",
    "        loss += tf.norm(tf.matmul(tf_W[i], tf_T[i-1]) - tf_A) \n",
    "\n",
    "      # Optimizer.\n",
    "      # We are going to find the minimum of this loss using gradient descent.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "   \n",
    "    # Run training\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # This is a one-time operation which ensures the parameters get initialized as\n",
    "      # we described in the graph\n",
    "      tf.global_variables_initializer().run()\n",
    "      print('Initialized')\n",
    "      for step in range(num_steps):\n",
    "        # Run the computations\n",
    "        _, l, T1, T, A = session.run([optimizer, loss, tf_T1, tf_T, tf_A])\n",
    "        if (step % 100 == 0):\n",
    "          print('Loss at step %d: %f' % (step, l))\n",
    "\n",
    "      # Print transformation matrices + universal embedding\n",
    "      print('\\n')\n",
    "      print('Transform 1:')\n",
    "      print(T1)\n",
    "      for i in range(0, T.shape[0]):\n",
    "          print('Transform {}:'.format(i+2))\n",
    "          print(T[i])\n",
    "      print('Universal embedding:')\n",
    "      print(A)\n",
    "\n",
    "    # Print transformed embeddings\n",
    "    print('\\n')\n",
    "    print('W1*T1:')\n",
    "    print(np.dot(W[0], T1))\n",
    "    for i in range(0, T.shape[0]):\n",
    "      print('W{0}*T{0}:'.format(i+2))\n",
    "      print(np.dot(W[i+1], T[i])) \n",
    "        \n",
    "    return (T1, T, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(swadesh_file, swad_idx, embed_file):\n",
    "    # Read swadesh list\n",
    "    ls_swad = []\n",
    "    ls_swad_full = []\n",
    "    n_found_i = []\n",
    "    with open(swadesh_file) as f:\n",
    "        ls_swad = []\n",
    "        lines = f.read().decode('utf-8').splitlines()\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i not in swad_idx:\n",
    "                found = False\n",
    "                if line != '':\n",
    "                    words = line.split('\\t')\n",
    "                    for word in words:\n",
    "                        if ' ' not in word:\n",
    "                            ls_swad.append(word.lower())\n",
    "                            ls_swad_full.append(word.lower())\n",
    "                            found = True\n",
    "                            break\n",
    "            if not found:\n",
    "                n_found_i.append(i)\n",
    "                ls_swad_full.append('NOT_FOUND')\n",
    "\n",
    "    print('Not found list len: {0}'.format(len(n_found_i)))\n",
    "    print(ls_swad)\n",
    "    print(len(ls_swad))\n",
    "\n",
    "    # Read embeddings\n",
    "    words = []\n",
    "    embedding_raw = []\n",
    "    embed_found_i = []\n",
    "    with open(embed_file) as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            fields = line.strip().decode('utf-8').split(' ')\n",
    "            w = fields[0]\n",
    "            w = w.lower()\n",
    "            if w in ls_swad:\n",
    "                embed_found_i.append(ls_swad_full.index(w))\n",
    "                # print(len(embed_found_i))\n",
    "                trans = fields[1:]\n",
    "                words.append(w)\n",
    "                embedding_raw.append(trans)\n",
    "                if i == len(ls_swad):\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "    # Delete not found embeddings from swadesh\n",
    "    # 1. calc not found indices\n",
    "    # 2. update not found index list\n",
    "    # 3. update swadesh list\n",
    "    n_found_i = np.sort(list(set(range(len(ls_swad_full))) - set(embed_found_i)))\n",
    "    ls_swad = np.delete(ls_swad_full, n_found_i)\n",
    "\n",
    "    print('Embeddings len: {0}'.format(len(embedding_raw)))\n",
    "    print('Not found: {0}\\n{1}'.format(len(n_found_i), n_found_i))\n",
    "\n",
    "    # Reorder embedding\n",
    "    idx_arr = [words.index(w) for w in ls_swad]\n",
    "    words_ordered = np.array(words)[idx_arr]\n",
    "    embedding_ordered = np.array(embedding_raw)[idx_arr]\n",
    "\n",
    "    # Normalize embedding\n",
    "    embedding = normalize(embedding_ordered.astype(np.float32))\n",
    "\n",
    "    return ls_swad, embedding, n_found_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_corr(embedding, swadesh):\n",
    "    cnt = embedding.shape[0]\n",
    "    corr_mx = np.ndarray(shape=(cnt, cnt), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, cnt):\n",
    "        for j in range(0, i + 1):\n",
    "            sim = cosine_similarity(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))\n",
    "            corr_mx[i][j] = sim\n",
    "            corr_mx[j][i] = sim\n",
    "    sim_mx_args = np.argsort(-corr_mx)\n",
    "    sims = {}\n",
    "    for i, w in enumerate(swadesh):\n",
    "        sims[w] = [swadesh[j] for j in sim_mx_args[i, :]]\n",
    "    return corr_mx, sim_mx_args, sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
