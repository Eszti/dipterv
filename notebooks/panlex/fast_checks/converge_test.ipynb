{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_emb = '/mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec'\n",
    "ita_emb = '/mnt/permanent/Language/Multi/FB/wiki.it/wiki.it.vec'\n",
    "limit = None\n",
    "limits_tr = [i*50000 for i in range(1, 16)]\n",
    "limits_te = [i*50000 for i in range(1, 41)]\n",
    "\n",
    "train_fn = '/home/eszti/projects/dipterv/panlex/data/smith/train/eng_ita.tsv'\n",
    "test_fn = '/home/eszti/projects/dipterv/panlex/data/smith/test/eng_ita.tsv'\n",
    "\n",
    "figsize_x = 8\n",
    "figsize_y = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_emb(emb_fn, limit):\n",
    "    model = KeyedVectors.load_word2vec_format(emb_fn, binary=False, limit=limit)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_pairs_tsv(fn, id1, id2, header=True):\n",
    "    wl1 = set()\n",
    "    wl2 = set()\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()\n",
    "        data = [(line.split()[id1], line.split()[id2]) for i, line in enumerate(lines) if i > 0 or header == False]\n",
    "    for (w1, w2) in data:\n",
    "        wl1.add(w1)\n",
    "        wl2.add(w2)\n",
    "    return data, wl1, wl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_emb_coverage(emb, wl, limits):\n",
    "    found = [[] for i in limits]    \n",
    "    for w in wl:\n",
    "        n_from = 0\n",
    "        n_to = len(limits)\n",
    "        if w in emb:\n",
    "            idx = emb.index2word.index(w)\n",
    "            for i, l in enumerate(limits):\n",
    "                if idx > l:\n",
    "                    n_from = i + 1\n",
    "        else:\n",
    "            # do not add any of the found lists\n",
    "            n_from = len(limits)\n",
    "        for i in range(n_from, n_to):\n",
    "            found[i].append(w)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wp_coverage(wp_l, found1, found2, limits):\n",
    "    found = [[] for i in limits]    \n",
    "    for (w1, w2) in wp_l:\n",
    "        for i, l in enumerate(limits):\n",
    "            if w1 in found1[i] and w2 in found2[i]:\n",
    "                found[i].append((w1, w2))\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, title, xlab, ylab):\n",
    "    plt.figure(figsize=(figsize_x,figsize_y))    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.grid()\n",
    "    plt.plot(x, y, 'o')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_en = read_emb(eng_emb, limit)\n",
    "m_it = read_emb(ita_emb, limit)\n",
    "len(m_en.index2word)\n",
    "len(m_it.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wp, tr_en, tr_it = read_word_pairs_tsv(train_fn, 0, 1)\n",
    "test_wp, te_en, te_it = read_word_pairs_tsv(test_fn, 0, 1)\n",
    "len(train_wp)\n",
    "len(tr_en)\n",
    "len(tr_it)\n",
    "len(test_wp)\n",
    "len(te_en)\n",
    "len(te_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_tr_en = check_emb_coverage(emb=m_en, wl=tr_en, limits=limits_tr)\n",
    "found_tr_it = check_emb_coverage(emb=m_it, wl=tr_it, limits=limits_tr)\n",
    "\n",
    "stat_tr_en = []\n",
    "stat_tr_it = []\n",
    "\n",
    "print('train en : {}'.format(len(tr_en)))\n",
    "for i, l in enumerate(limits_tr):\n",
    "    y = len(found_tr_en[i])\n",
    "    stat_tr_en.append(y)\n",
    "    print('limit - {0} : {1}'.format(l, y))\n",
    "print('not found: {}'.format(set(tr_en) - set(found_tr_en[-1])))\n",
    "plot(limits_tr, stat_tr_en, 'train en', 'limits', 'found')\n",
    "\n",
    "print('train it : {}'.format(len(tr_it)))\n",
    "for i, l in enumerate(limits_tr):\n",
    "    y = len(found_tr_it[i])\n",
    "    stat_tr_it.append(y)\n",
    "    print('limit - {0} : {1}'.format(l, y))\n",
    "print('not found: {}'.format(set(tr_it) - set(found_tr_it[-1])))\n",
    "plot(limits_tr, stat_tr_it, 'train it', 'limits', 'found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proving that not found words are not in the embedding\n",
    "'prelaurea' in m_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_te_en = check_emb_coverage(emb=m_en, wl=te_en, limits=limits_te)\n",
    "found_te_it = check_emb_coverage(emb=m_it, wl=te_it, limits=limits_te)\n",
    "\n",
    "stat_te_en = []\n",
    "stat_te_it = []\n",
    "\n",
    "print('test en : {}'.format(len(te_en)))\n",
    "for i, l in enumerate(limits_te):\n",
    "    y = len(found_te_en[i])\n",
    "    stat_te_en.append(y)\n",
    "    print('limit - {0} : {1}'.format(l, y))\n",
    "print('not found: {}'.format(set(te_en) - set(found_te_en[-1])))\n",
    "plot(limits_te, stat_te_en, 'test en', 'limits', 'found')\n",
    "\n",
    "print('test it : {}'.format(len(te_it)))\n",
    "for i, l in enumerate(limits_te):\n",
    "    y = len(found_te_it[i])\n",
    "    stat_te_it.append(y)\n",
    "    print('limit - {0} : {1}'.format(l, y))\n",
    "print('not found: {}'.format(set(te_it) - set(found_te_it[-1])))\n",
    "plot(limits_te, stat_te_it, 'test it', 'limits', 'found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proving that not found words are not in the embedding\n",
    "'ridimensioni' in m_it\n",
    "'kostunica' in m_it\n",
    "'oligopolistica' in m_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_tr_wp = check_wp_coverage(train_wp, found_tr_en, found_tr_it, limits_tr)\n",
    "found_te_wp = check_wp_coverage(test_wp, found_te_en, found_te_it, limits_te)\n",
    "\n",
    "stat_tr_wp = []\n",
    "stat_te_wp = []\n",
    "\n",
    "print('train : {}'.format(len(train_wp)))\n",
    "for i, l in enumerate(limits_tr):\n",
    "    y = len(found_tr_wp[i])\n",
    "    stat_tr_wp.append(y)\n",
    "    print('limit - {0} : {1}'.format(l, y))\n",
    "print('not found: {}'.format(set(train_wp) - set(found_tr_wp[-1])))\n",
    "plot(limits_tr, stat_tr_wp, 'train word pairs', 'limits', 'found')\n",
    "\n",
    "print('test : {}'.format(len(test_wp)))\n",
    "for i, l in enumerate(limits_te):\n",
    "    y = len(found_te_wp[i])\n",
    "    stat_te_wp.append(y)\n",
    "    print('limit - {0} : {1}'.format(l, y))\n",
    "print('not found: {}'.format(set(test_wp) - set(found_te_wp[-1])))\n",
    "plot(limits_te, stat_te_wp, 'test word pairs', 'limits', 'found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all():\n",
    "    plt.figure(figsize=(figsize_x, figsize_y))\n",
    "    plt.title('train')\n",
    "    plt.xlabel('limit')\n",
    "    plt.ylabel('found')\n",
    "    plt.grid()\n",
    "    plt.plot(limits_tr, stat_tr_en, 'ro', label='en: {}'.format(len(tr_en)))\n",
    "    plt.plot(limits_tr, stat_tr_it, 'bo', label='it: {}'.format(len(tr_it)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(figsize_x, figsize_y))\n",
    "    plt.title('test')\n",
    "    plt.xlabel('limit')\n",
    "    plt.ylabel('found')\n",
    "    plt.grid()\n",
    "    plt.plot(limits_te, stat_te_en, 'ro', label='en: {}'.format(len(te_en)))\n",
    "    plt.plot(limits_te, stat_te_it, 'bo', label='it: {}'.format(len(te_it)))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(figsize_x,figsize_y))\n",
    "    plt.title('word pairs')\n",
    "    plt.xlabel('limit')\n",
    "    plt.ylabel('found')\n",
    "    plt.grid()\n",
    "    plt.plot(limits_tr, stat_tr_wp, 'ro', label='train: {}'.format(len(train_wp)))\n",
    "    plt.plot(limits_te, stat_te_wp, 'bo', label='test: {}'.format(len(test_wp)))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
