{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load our data\n",
    "\n",
    "( {lang : swad_list, emb_full (norm), emb_fn, not_found_list, T}, univ(norm)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec\n",
      "/mnt/permanent/Language/Multi/FB/wiki.it/wiki.it.vec\n"
     ]
    }
   ],
   "source": [
    "# Path our data \n",
    "pickle_path = '/home/eszti/projects/recap/find_univ_proc/find_univ_proc.pickle'\n",
    "\n",
    "with open(pickle_path) as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Our data\n",
    "eng_data = data[0]['eng']\n",
    "ita_data = data[0]['ita']\n",
    "\n",
    "# Swadesh words used for training\n",
    "sw_en = eng_data[0]\n",
    "sw_it = ita_data[0]\n",
    "\n",
    "# Embed paths\n",
    "eng_emb_fn = eng_data[2]\n",
    "ita_emb_fn = ita_data[2]\n",
    "print(eng_emb_fn)\n",
    "print(ita_emb_fn)\n",
    "\n",
    "# Not found lists\n",
    "sw_nf_en = eng_data[3]\n",
    "sw_nf_it = ita_data[3]\n",
    "\n",
    "# Transformation mx-s\n",
    "T_en = eng_data[4]\n",
    "T_it = ita_data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load Smith's test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_pairs_path = '/home/eszti/projects/smith/transmat/data/OPUS_en_it_europarl_test.txt'\n",
    "en_it_dict = dict()\n",
    "it_en_dict = dict()\n",
    "\n",
    "with open(test_pairs_path) as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        words = line.strip().decode('utf-8').split(' ')\n",
    "        en = words[0]\n",
    "        it = words[1]\n",
    "        if en not in en_it_dict.keys():\n",
    "            en_it_dict[en] = []\n",
    "        en_it_dict[en].append(it)\n",
    "        if it not in it_en_dict.keys():\n",
    "            it_en_dict[it] = []\n",
    "        it_en_dict[it].append(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En-It len: 1500\n",
      "It-En len: 1849\n"
     ]
    }
   ],
   "source": [
    "print('En-It len: {}'.format(len(en_it_dict)))\n",
    "print('It-En len: {}'.format(len(it_en_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load our previosly selected set embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found in English: 0\n",
      "Not found in Italian: 3\n"
     ]
    }
   ],
   "source": [
    "fb_emb_data_path = 'data/our_data_raw_09_29.pickle'\n",
    "\n",
    "with open(fb_emb_data_path) as f:\n",
    "    fb_emb_data = pickle.load(f)\n",
    "    \n",
    "(wl_en, emb_en, id_en) = fb_emb_data[0]\n",
    "(wl_it, emb_it, id_it) = fb_emb_data[1]\n",
    "\n",
    "print('Not found in English: {}'.format(len(wl_en) - len(id_en)))\n",
    "print('Not found in Italian: {}'.format(len(wl_it) - len(id_it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_not_found_words_idx(wl, idxs):\n",
    "    nf_i = []\n",
    "    nf_w = []\n",
    "    for i in range(len(wl)):\n",
    "        if i not in idxs:\n",
    "            nf_i.append(i)\n",
    "            nf_w.append(wl[i])\n",
    "    return nf_i, nf_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nf_en_i, nf_en_w = get_not_found_words_idx(wl_en, id_en)\n",
    "nf_it_i, nf_it_w = get_not_found_words_idx(wl_it, id_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build excluded words list\n",
    "- get found swadesh words (words from Swadesh used during training)\n",
    "- get not found embeddings (words from Smith's test set)\n",
    "- concat the two list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ex_words(swad_list, nf_idxs, nf_embeddings):\n",
    "    # get found swadesh words, they were used for training\n",
    "    ex_words = []\n",
    "    for i, w in enumerate(swad_list):\n",
    "        if i not in nf_idxs:\n",
    "            ex_words.append(w)\n",
    "    # get not found embedding\n",
    "    ex_words += nf_embeddings\n",
    "    return ex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get filtered dictionary\n",
    "def get_filt_dict(d1, ex1, ex2):\n",
    "    fd1 = dict()\n",
    "    for k, vs in d1.iteritems():\n",
    "        if k in ex1:\n",
    "            print('\"{}\" is removed because it is in ex1'.format(k))\n",
    "            continue\n",
    "        zero_stays = True\n",
    "        for v in vs:\n",
    "            if v not in ex2:\n",
    "                zero_stays = False\n",
    "        if zero_stays:\n",
    "            print('\"{}\" is removed because all values \"{}\" are in ex2'.format(k, vs))\n",
    "            continue\n",
    "        fd1[k] = vs\n",
    "    print('\\n')\n",
    "    return fd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"green\" is removed because it is in ex1\n",
      "\"downsize\" is removed because all values \"[u'ridimensioni']\" are in ex2\n",
      "\"head\" is removed because it is in ex1\n",
      "\"cold\" is removed because it is in ex1\n",
      "\"woman\" is removed because all values \"[u'donna']\" are in ex2\n",
      "\"kostunica\" is removed because all values \"[u'kostunica']\" are in ex2\n",
      "\"close\" is removed because it is in ex1\n",
      "\"sole\" is removed because all values \"[u'sole']\" are in ex2\n",
      "\"oligopolistic\" is removed because all values \"[u'oligopolistica']\" are in ex2\n",
      "\"donna\" is removed because all values \"[u'donna']\" are in ex2\n",
      "\"bird\" is removed because it is in ex1\n",
      "\"red\" is removed because it is in ex1\n",
      "\"neighbour\" is removed because all values \"[u'vicino']\" are in ex2\n",
      "\n",
      "\n",
      "\"vicino\" is removed because it is in ex1\n",
      "\"capo\" is removed because it is in ex1\n",
      "\"oligopolistica\" is removed because it is in ex1\n",
      "\"stretta\" is removed because all values \"[u'close']\" are in ex2\n",
      "\"verdi\" is removed because all values \"[u'green']\" are in ex2\n",
      "\"mano\" is removed because it is in ex1\n",
      "\"kostunica\" is removed because it is in ex1\n",
      "\"rosso\" is removed because it is in ex1\n",
      "\"fredda\" is removed because all values \"[u'cold']\" are in ex2\n",
      "\"verde\" is removed because it is in ex1\n",
      "\"grande\" is removed because it is in ex1\n",
      "\"sole\" is removed because it is in ex1\n",
      "\"donna\" is removed because it is in ex1\n",
      "\"ridimensioni\" is removed because it is in ex1\n",
      "\"freddo\" is removed because it is in ex1\n",
      "\"uccelli\" is removed because all values \"[u'bird']\" are in ex2\n",
      "\"luna\" is removed because it is in ex1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# English words to delete\n",
    "ex_en = get_ex_words(sw_en, sw_nf_en, nf_en_w)\n",
    "\n",
    "# Italian words to delete\n",
    "ex_it = get_ex_words(sw_it, sw_nf_it, nf_it_w)\n",
    "\n",
    "# Get filtered d1\n",
    "fd_en = get_filt_dict(en_it_dict, ex_en, ex_it)\n",
    "# Get filtered d2\n",
    "fd_it = get_filt_dict(it_en_dict, ex_it, ex_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len orig en-it: 1500\n",
      "Len filt en-it: 1487\n",
      "Len orig it-en: 1849\n",
      "Len filt it-en: 1832\n"
     ]
    }
   ],
   "source": [
    "print('Len orig en-it: {}'.format(len(en_it_dict)))\n",
    "print('Len filt en-it: {}'.format(len(fd_en)))\n",
    "print('Len orig it-en: {}'.format(len(it_en_dict)))\n",
    "print('Len filt it-en: {}'.format(len(fd_it)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding matrix\n",
    "- empty np array with proper size\n",
    "- insert embeddings line by line\n",
    "- in parallel create emb_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_np_emb(wl, emb_l, id_emb, allowed, rows, cols):\n",
    "    emb = np.zeros(shape=(rows, cols))\n",
    "    emb_idx = 0\n",
    "    emb_wl = []\n",
    "    for i, w in enumerate(wl):\n",
    "        if emb_l[i] is None:\n",
    "            print('exclude: \"{}\", embed not found'.format(w))\n",
    "            continue\n",
    "        if w not in allowed:\n",
    "            print('exclude: \"{}\", used for training'.format(w))\n",
    "            continue\n",
    "        emb[emb_idx, :] = emb_l[i]\n",
    "        emb_idx += 1\n",
    "        emb_wl.append(w)\n",
    "    print('\\n')\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude: \"green\", used for training\n",
      "exclude: \"downsize\", used for training\n",
      "exclude: \"head\", used for training\n",
      "exclude: \"cold\", used for training\n",
      "exclude: \"woman\", used for training\n",
      "exclude: \"kostunica\", used for training\n",
      "exclude: \"close\", used for training\n",
      "exclude: \"sole\", used for training\n",
      "exclude: \"oligopolistic\", used for training\n",
      "exclude: \"donna\", used for training\n",
      "exclude: \"bird\", used for training\n",
      "exclude: \"red\", used for training\n",
      "exclude: \"neighbour\", used for training\n",
      "\n",
      "\n",
      "exclude: \"vicino\", used for training\n",
      "exclude: \"capo\", used for training\n",
      "exclude: \"oligopolistica\", embed not found\n",
      "exclude: \"stretta\", used for training\n",
      "exclude: \"verdi\", used for training\n",
      "exclude: \"mano\", used for training\n",
      "exclude: \"kostunica\", embed not found\n",
      "exclude: \"rosso\", used for training\n",
      "exclude: \"fredda\", used for training\n",
      "exclude: \"verde\", used for training\n",
      "exclude: \"grande\", used for training\n",
      "exclude: \"sole\", used for training\n",
      "exclude: \"donna\", used for training\n",
      "exclude: \"ridimensioni\", embed not found\n",
      "exclude: \"freddo\", used for training\n",
      "exclude: \"uccelli\", used for training\n",
      "exclude: \"luna\", used for training\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_np_emb = get_np_emb(wl_en, emb_en, id_en, fd_en.keys(), len(fd_en), 300)\n",
    "\n",
    "it_np_emb = get_np_emb(wl_it, emb_it, id_it, fd_it.keys(), len(fd_it), 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate\n",
    "# (Wen*Ten)*Tit'\n",
    "en_it_np_emb = np.dot(np.dot(en_np_emb, T_en), np.transpose(T_it))\n",
    "# (Wit*Tit)*Ten'\n",
    "it_en_np_emb = np.dot(np.dot(it_np_emb, T_it), np.transpose(T_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
