{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import copy\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf\n",
    "langs = ['eng', 'ita']\n",
    "dim = 300\n",
    "\n",
    "\n",
    "sil2fb_fn = '/home/eszti/projects/dipterv/notebooks/panlex/data/sil2fb.json'\n",
    "\n",
    "with open(sil2fb_fn) as f:\n",
    "     sil2fb = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding from /mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec\n",
      "Embedding for eng is read\n",
      "Reading embedding from /mnt/permanent/Language/Multi/FB/wiki.it/wiki.it.vec\n",
      "Embedding for ita is read\n"
     ]
    }
   ],
   "source": [
    "# Read embeddings\n",
    "\n",
    "fold = '/mnt/permanent/Language/Multi/FB' \n",
    "\n",
    "d_models = dict()\n",
    "for l in langs:\n",
    "    fn = os.path.join(fold, 'wiki.{}'.format(sil2fb[l]), 'wiki.{}.vec'.format(sil2fb[l]))\n",
    "    print('Reading embedding from {}'.format(fn))\n",
    "    model = KeyedVectors.load_word2vec_format(fn, binary=False)\n",
    "    model.syn0 /= np.sqrt((model.syn0**2).sum(1))[:, None]\n",
    "    d_models[l] = model\n",
    "    print('Embedding for {} is read'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading eng-ita dictionary\n",
      "Words not found in embedding: []\n",
      "Reading ita-eng dictionary\n",
      "Words not found in embedding: ['kostunica', 'ridimensioni', 'oligopolistica']\n"
     ]
    }
   ],
   "source": [
    "# Read word pairs from tsv\n",
    "\n",
    "def read_word_pairs_tsv(fn, id1, id2, header=True):\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()\n",
    "        data = [(line.split()[id1], line.split()[id2]) for i, line in enumerate(lines) if i > 0 or header == False]\n",
    "    return data\n",
    "\n",
    "def wp_list_2_dict(lang_pair, wp_l):\n",
    "    l1 = lang_pair[0]\n",
    "    l2 = lang_pair[1]\n",
    "    l12 = dict()\n",
    "    l21 = dict()\n",
    "    for (w1, w2) in wp_l:\n",
    "        if w1 not in l12:\n",
    "            l12[w1] = [w2]\n",
    "        else:\n",
    "            l12[w1].append(w2)\n",
    "        if w2 not in l21:\n",
    "            l21[w2] = [w1]\n",
    "        else:\n",
    "            l21[w2].append(w1)\n",
    "    return l12, l21\n",
    "\n",
    "fold = '/home/eszti/projects/dipterv/notebooks/panlex/smith/test'\n",
    "id1 = 0\n",
    "id2 = 1\n",
    "\n",
    "# Dict for word pairs\n",
    "d_wps = dict()\n",
    "done = set()\n",
    "for lang1 in langs:\n",
    "    for lang2 in langs:\n",
    "        lang_pair = tuple(sorted([lang1, lang2]))\n",
    "        if lang1 == lang2 or lang_pair in done:\n",
    "            continue\n",
    "        done.add(lang_pair)\n",
    "        l1 = lang_pair[0]\n",
    "        l2 = lang_pair[1]\n",
    "        fn = os.path.join(fold, '{0}_{1}.tsv'.format(l1, l2))\n",
    "        data = read_word_pairs_tsv(fn, id1, id2, False)\n",
    "        d_wps[lang_pair] = data\n",
    "\n",
    "# Dict for dictionaries between each languages\n",
    "d_dict = dict()\n",
    "for ((l1, l2), wp_l) in d_wps.items():\n",
    "    l12, l21 = wp_list_2_dict((l1, l2), wp_l)\n",
    "    d_dict[(l1, l2)] = l12\n",
    "    d_dict[(l2, l1)] = l21\n",
    "    \n",
    "# Dict for filtered models containing only the words used for training\n",
    "d_tr_mods = dict()\n",
    "for ((l1, l2), d) in d_dict.items():\n",
    "    print('Reading {0}-{1} dictionary'.format(l1, l2))\n",
    "    tr_mod = KeyedVectors()\n",
    "    nf_list = []\n",
    "    for i, w in enumerate(list(d.keys())):\n",
    "        # Check if there's an embedding to the word\n",
    "        if w not in d_models[l1]:\n",
    "            nf_list.append(w)\n",
    "    print('Words not found in embedding: {}'.format(nf_list))\n",
    "    tr_mod.index2word = [x for x in list(d.keys()) if x not in nf_list]\n",
    "    tr_mod.syn0 = np.ndarray(shape=(len(tr_mod.index2word), dim), dtype=np.float32)\n",
    "    # Adding embedding to train model\n",
    "    for i, w in enumerate(tr_mod.index2word):\n",
    "        tr_mod.syn0[i, :] = d_models[l1][w]\n",
    "    # Deleting not forund words from word pairs list\n",
    "    change = False\n",
    "    if l1 < l2:\n",
    "        lang1 = l1; lang2 = l2\n",
    "    else:\n",
    "        lang1 = l2; lang2 = l1; change = True\n",
    "    d_wps[(lang1, lang2)] = [(w1, w2) for (w1, w2) in d_wps[(lang1, lang2)] \n",
    "                             if not ((change and w2 in nf_list) or (not change and w1 in nf_list))]\n",
    "    d_tr_mods[(l1, l2)] = tr_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1866"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1849"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "871053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "871053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1846"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "619"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_wps[('eng', 'ita')])\n",
    "len(d_dict[('eng', 'ita')])\n",
    "len(d_dict[('ita', 'eng')])\n",
    "len(d_models['eng'].index2word)\n",
    "len(d_models['eng'].syn0)\n",
    "len(d_models['ita'].index2word)\n",
    "len(d_models['ita'].syn0)\n",
    "len(d_tr_mods[('eng', 'ita')].syn0)\n",
    "len(d_tr_mods[('ita', 'eng')].syn0)\n",
    "\n",
    "d_tr_mods[('eng', 'ita')].index2word.index('kostunica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate precision\n",
    "\n",
    "# model_src : source language embeddings (need to have syn0 and index2word properites) (after translation)\n",
    "# model_tar : target language embeddings (need to have syn0 and index2word properites) (can be don in orig or universal space)\n",
    "# dict_scr_2_tar : dictionary from source to target\n",
    "def calc_precision(precs, model_src, model_tar, dict_scr_2_tar, verbose=False):\n",
    "    W_src = model_src.syn0\n",
    "    W_tar = model_tar.syn0\n",
    "    idx_src = model_src.index2word\n",
    "    idx_tar = model_tar.index2word\n",
    "    \n",
    "    cos_mx = cosine_similarity(W_src, W_tar)\n",
    "    sim_mx = np.argsort(-cos_mx)\n",
    "    max_prec = max(precs)\n",
    "    prec_cnt = np.zeros(shape=(1, max_prec))\n",
    "    if verbose:\n",
    "        print('word: \\ttranslations in dict: \\tclosest words after translation: \\t')\n",
    "    for i, r in enumerate(sim_mx):\n",
    "        key_word = idx_src[i]\n",
    "        value_words = dict_scr_2_tar[key_word]\n",
    "        closest_words = []\n",
    "        for j in range(max_prec):       \n",
    "            ans = np.where(r==j)\n",
    "            idx_orig = ans[0][0]\n",
    "            word = idx_tar[idx_orig]\n",
    "            closest_words.append(word)\n",
    "            if word in value_words:\n",
    "                prec_cnt[0][j] = prec_cnt[0][j] + 1\n",
    "        if verbose:\n",
    "            print('{}\"\\t{}\\t{}'.format(key_word, value_words, closest_words))\n",
    "    if verbose:\n",
    "        print(prec_cnt)\n",
    "    prec_pcnts = []\n",
    "    for i, val in enumerate(precs):\n",
    "        sum_hit = np.sum(prec_cnt[0][0:val])\n",
    "        pcnt = float(sum_hit)/sim_mx.shape[0]\n",
    "        if verbose:\n",
    "            print('prec {} : {}'.format(val, pcnt))\n",
    "        prec_pcnts.append(pcnt)\n",
    "    return prec_pcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read T mx from file\n",
    "fn = ''\n",
    "\n",
    "with open(fn) as f:\n",
    "    nzpf = np.load(f)\n",
    "    T = nzpf['T']\n",
    "    \n",
    "T1 = T[0]\n",
    "T2 = T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 'eng'\n",
    "l2 = 'ita'\n",
    "m1_tr = d_tr_mods[(l1, l2)]\n",
    "m2_tr = d_tr_mods[(l2, l1)]\n",
    "m1 = d_models[l1]\n",
    "m2 = d_models[l2]\n",
    "# Prec l1 - l2 = eng - ita\n",
    "m1_tr.syn0 = np.dot(m1_tr.syn0, T1)\n",
    "m2.syn0 = np.dot(m2.syn0, T2)\n",
    "precs_1 = calc_precision(precs_to_calc, m1_tr, m2, d_dict[(l1, l2)], verbose=False)\n",
    "precs_1\n",
    "# Prec l2 - l1 = ita - eng\n",
    "m2_tr.syn0 = np.dot(m2_tr.syn0, T2)\n",
    "m1.syn0 = np.dot(m1.syn0, T1)\n",
    "precs_2 = calc_precision(precs_to_calc, m2_tr, m1, d_dict[(l2, l1)], verbose=False)\n",
    "precs_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
