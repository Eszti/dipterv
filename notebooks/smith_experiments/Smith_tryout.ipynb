{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import copy\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf\n",
    "langs = ['eng', 'ita']\n",
    "dim = 300\n",
    "\n",
    "\n",
    "sil2fb_fn = '/home/eszti/projects/dipterv/notebooks/panlex/data/sil2fb.json'\n",
    "\n",
    "with open(sil2fb_fn) as f:\n",
    "     sil2fb = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding from /mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec\n",
      "Embedding for eng is read\n",
      "Reading embedding from /mnt/permanent/Language/Multi/FB/wiki.it/wiki.it.vec\n",
      "Embedding for ita is read\n"
     ]
    }
   ],
   "source": [
    "# Read embeddings\n",
    "\n",
    "fold = '/mnt/permanent/Language/Multi/FB' \n",
    "\n",
    "d_models = dict()\n",
    "for l in langs:\n",
    "    fn = os.path.join(fold, 'wiki.{}'.format(sil2fb[l]), 'wiki.{}.vec'.format(sil2fb[l]))\n",
    "    print('Reading embedding from {}'.format(fn))\n",
    "    model = KeyedVectors.load_word2vec_format(fn, binary=False)\n",
    "    model.syn0 /= np.sqrt((model.syn0**2).sum(1))[:, None]\n",
    "    d_models[l] = model\n",
    "    print('Embedding for {} is read'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading eng-ita dictionary\n",
      "Words not found in embedding: []\n",
      "Reading ita-eng dictionary\n",
      "Words not found in embedding: ['prelaurea']\n"
     ]
    }
   ],
   "source": [
    "# Read word pairs from tsv\n",
    "\n",
    "def read_word_pairs_tsv(fn, id1, id2, header=True):\n",
    "    with open(fn) as f:\n",
    "        lines = f.readlines()\n",
    "        data = [(line.split()[id1], line.split()[id2]) for i, line in enumerate(lines) if i > 0 or header == False]\n",
    "    return data\n",
    "\n",
    "def wp_list_2_dict(lang_pair, wp_l):\n",
    "    l1 = lang_pair[0]\n",
    "    l2 = lang_pair[1]\n",
    "    l12 = dict()\n",
    "    l21 = dict()\n",
    "    for (w1, w2) in wp_l:\n",
    "        if w1 not in l12:\n",
    "            l12[w1] = [w2]\n",
    "        else:\n",
    "            l12[w1].append(w2)\n",
    "        if w2 not in l21:\n",
    "            l21[w2] = [w1]\n",
    "        else:\n",
    "            l21[w2].append(w1)\n",
    "    return l12, l21\n",
    "\n",
    "fold = '/home/eszti/projects/dipterv/notebooks/panlex/smith/'\n",
    "id1 = 0\n",
    "id2 = 1\n",
    "\n",
    "# Dict for word pairs\n",
    "d_wps = dict()\n",
    "done = set()\n",
    "for lang1 in langs:\n",
    "    for lang2 in langs:\n",
    "        lang_pair = tuple(sorted([lang1, lang2]))\n",
    "        if lang1 == lang2 or lang_pair in done:\n",
    "            continue\n",
    "        done.add(lang_pair)\n",
    "        l1 = lang_pair[0]\n",
    "        l2 = lang_pair[1]\n",
    "        fn = os.path.join(fold, '{0}_{1}.tsv'.format(l1, l2))\n",
    "        data = read_word_pairs_tsv(fn, id1, id2, False)\n",
    "        d_wps[lang_pair] = data\n",
    "\n",
    "# Dict for dictionaries between each languages\n",
    "d_dict = dict()\n",
    "for ((l1, l2), wp_l) in d_wps.items():\n",
    "    l12, l21 = wp_list_2_dict((l1, l2), wp_l)\n",
    "    d_dict[(l1, l2)] = l12\n",
    "    d_dict[(l2, l1)] = l21\n",
    "    \n",
    "# Dict for filtered models containing only the words used for training\n",
    "d_tr_mods = dict()\n",
    "for ((l1, l2), d) in d_dict.items():\n",
    "    print('Reading {0}-{1} dictionary'.format(l1, l2))\n",
    "    tr_mod = KeyedVectors()\n",
    "    nf_list = []\n",
    "    for i, w in enumerate(list(d.keys())):\n",
    "        # Check if there's an embedding to the word\n",
    "        if w not in d_models[l1]:\n",
    "            nf_list.append(w)\n",
    "    print('Words not found in embedding: {}'.format(nf_list))\n",
    "    tr_mod.index2word = [x for x in list(d.keys()) if x not in nf_list]\n",
    "    tr_mod.syn0 = np.ndarray(shape=(len(tr_mod.index2word), dim), dtype=np.float32)\n",
    "    # Adding embedding to train model\n",
    "    for i, w in enumerate(tr_mod.index2word):\n",
    "        tr_mod.syn0[i, :] = d_models[l1][w]\n",
    "    # Deleting not forund words from word pairs list\n",
    "    change = False\n",
    "    if l1 < l2:\n",
    "        lang1 = l1; lang2 = l2\n",
    "    else:\n",
    "        lang1 = l2; lang2 = l1; change = True\n",
    "    d_wps[(lang1, lang2)] = [(w1, w2) for (w1, w2) in d_wps[(lang1, lang2)] \n",
    "                             if not ((change and w2 in nf_list) or (not change and w1 in nf_list))]\n",
    "    d_tr_mods[(l1, l2)] = tr_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3442"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4549"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "871053"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "871053"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3442"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4548"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_wps[('eng', 'ita')])\n",
    "len(d_dict[('eng', 'ita')])\n",
    "len(d_dict[('ita', 'eng')])\n",
    "len(d_models['eng'].index2word)\n",
    "len(d_models['eng'].syn0)\n",
    "len(d_models['ita'].index2word)\n",
    "len(d_models['ita'].syn0)\n",
    "len(d_tr_mods[('eng', 'ita')].syn0)\n",
    "len(d_tr_mods[('ita', 'eng')].syn0)\n",
    "\n",
    "[(w1, w2) for (w1, w2) in d_wps[('eng', 'ita')] if w1 == 'prelaurea' or w2 == 'prelaurea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate precision\n",
    "\n",
    "# model_src : source language embeddings (need to have syn0 and index2word properites) (after translation)\n",
    "# model_tar : target language embeddings (need to have syn0 and index2word properites) (can be don in orig or universal space)\n",
    "# dict_scr_2_tar : dictionary from source to target\n",
    "def calc_precision(precs, model_src, model_tar, dict_scr_2_tar, verbose=False):\n",
    "    W_src = model_src.syn0\n",
    "    W_tar = model_tar.syn0\n",
    "    idx_src = model_src.index2word\n",
    "    idx_tar = model_tar.index2word\n",
    "    \n",
    "    cos_mx = cosine_similarity(W_src, W_tar)\n",
    "    sim_mx = np.argsort(-cos_mx)\n",
    "    max_prec = max(precs)\n",
    "    prec_cnt = np.zeros(shape=(1, max_prec))\n",
    "    if verbose:\n",
    "        print('word: \\ttranslations in dict: \\tclosest words after translation: \\t')\n",
    "    for i, r in enumerate(sim_mx):\n",
    "        key_word = idx_src[i]\n",
    "        value_words = dict_scr_2_tar[key_word]\n",
    "        closest_words = []\n",
    "        for j in range(max_prec):       \n",
    "            ans = np.where(r==j)\n",
    "            idx_orig = ans[0][0]\n",
    "            word = idx_tar[idx_orig]\n",
    "            closest_words.append(word)\n",
    "            if word in value_words:\n",
    "                prec_cnt[0][j] = prec_cnt[0][j] + 1\n",
    "        if verbose:\n",
    "            print('{}\"\\t{}\\t{}'.format(key_word, value_words, closest_words))\n",
    "    if verbose:\n",
    "        print(prec_cnt)\n",
    "    prec_pcnts = []\n",
    "    for i, val in enumerate(precs):\n",
    "        sum_hit = np.sum(prec_cnt[0][0:val])\n",
    "        pcnt = float(sum_hit)/sim_mx.shape[0]\n",
    "        if verbose:\n",
    "            print('prec {} : {}'.format(val, pcnt))\n",
    "        prec_pcnts.append(pcnt)\n",
    "    return prec_pcnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0011621150493898896, 0.0017431725740848344]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_precision([1, 3, 5], d_tr_mods[('eng', 'ita')], d_models['ita'], d_dict[('eng', 'ita')])\n",
    "calc_precision([1, 3, 5], d_tr_mods[('eng', 'ita')], d_tr_mods[('ita', 'eng')], d_dict[('eng', 'ita')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_precision([1, 3, 5], d_tr_mods[('ita', 'eng')], d_models['eng'], d_dict[('ita', 'eng')])\n",
    "calc_precision([1, 3, 5], d_tr_mods[('ita', 'eng')], d_tr_mods[('eng', 'ita')], d_dict[('ita', 'eng')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langs : list containing sil codes of languages, tf_T follows the same order\n",
    "# d_models : dictionary, lang - embedding (KeyedVectors)\n",
    "# d_wps : dictiorary, lang_pair - wordpair list\n",
    "# d_tr_mods: dictionary, lang_pair - embedding (KeyedVectors) only words used for training\n",
    "# dim : dimensiion of embedding\n",
    "# epochs : epochs to run\n",
    "# precs_to_calc : list of precisions to calculate, e.g. [1,3,5] if we want Precision @1, @3, @5\n",
    "# iters : optional, break after n update\n",
    "# lr : learning rate\n",
    "# svd : whether to do svd regularization\n",
    "# svd_f : how often regularize\n",
    "# verbose : print out details\n",
    "def train(langs, d_models, d_wps, d_tr_mods, dim, epochs, precs_to_calc, iters=None, lr=0.3, svd=False, svd_f=1, verbose=False):\n",
    "    nb_langs = len(langs)\n",
    "\n",
    "    # Init graphs\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # TF variables \n",
    "        # Placeholder for 2 words\n",
    "        tf_w1 = tf.placeholder(tf.float32, shape=[None, dim])\n",
    "        tf_w2 = tf.placeholder(tf.float32, shape=[None, dim])\n",
    "        # Placeholder for indexing the T matrix\n",
    "        tf_idx_l1 = tf.placeholder(tf.int32)\n",
    "        tf_idx_l2 = tf.placeholder(tf.int32)\n",
    "        # Translation matrices\n",
    "        tf_T = tf.Variable(tf.truncated_normal([nb_langs, dim, dim]))\n",
    "        \n",
    "        # SVD reguralization\n",
    "        tf_s1, tf_U1, tf_V1 = tf.svd(tf_T[tf_idx_l1], full_matrices=True, compute_uv=True)\n",
    "        updated_1 = tf.assign(tf_T[tf_idx_l1], tf.matmul(tf_U1, tf_V1))\n",
    "        tf_s2, tf_U2, tf_V2 = tf.svd(tf_T[tf_idx_l2], full_matrices=True, compute_uv=True)\n",
    "        updated_2 = tf.assign(tf_T[tf_idx_l2], tf.matmul(tf_U2, tf_V2))\n",
    "\n",
    "        # Loss\n",
    "        tf_T1 = tf.matmul(tf_w1, tf_T[tf_idx_l1])\n",
    "        tf_T2 = tf.matmul(tf_w2, tf_T[tf_idx_l2])\n",
    "        tf_T1_n = tf.nn.l2_normalize(tf_T1, dim=1)\n",
    "        tf_T2_n = tf.nn.l2_normalize(tf_T2, dim=1)\n",
    "        loss = tf.matmul(tf_T1_n, tf.transpose(tf_T2_n))\n",
    "        loss = -loss\n",
    "        \n",
    "        # Applying optimizer, Todo: try different optimizers!!\n",
    "        # https://www.tensorflow.org/api_guides/python/train#Optimizers \n",
    "        optimizer = tf.train.AdagradOptimizer(lr).minimize(loss)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        j = 0\n",
    "        lc_arr = []\n",
    "        precs_arr = []\n",
    "        for i in range(epochs):\n",
    "            loss_arr = []\n",
    "            for ((l1, l2), wp_l) in d_wps.items():\n",
    "                loss_arr_l = []\n",
    "                idx_l1 = langs.index(l1)\n",
    "                idx_l2 = langs.index(l2)\n",
    "                k = 0\n",
    "                for (w1, w2) in wp_l:\n",
    "                    emb1 = d_models[l1][w1].reshape((1, 300))\n",
    "                    emb2 = d_models[l2][w2].reshape((1, 300))\n",
    "                    # Todo: if we add \"or j == 0\" for some reason it's better in this mock example\n",
    "                    if (svd and i % svd_f == 0) or j == 0:\n",
    "                        _, l, _, _, T = session.run([optimizer, loss, updated_1, updated_2, tf_T], \n",
    "                                                      feed_dict={tf_w1 : emb1, \n",
    "                                                                 tf_w2 : emb2, \n",
    "                                                                 tf_idx_l1 : idx_l1, \n",
    "                                                                 tf_idx_l2 : idx_l2})\n",
    "                    else:\n",
    "                        _, l, T = session.run([optimizer, loss, tf_T],\n",
    "                                             feed_dict={tf_w1 : emb1, \n",
    "                                                        tf_w2 : emb2, \n",
    "                                                        tf_idx_l1 : idx_l1, \n",
    "                                                        tf_idx_l2 : idx_l2})\n",
    "                    j += 1\n",
    "                    k += 1\n",
    "                    loss_arr.append(-l[0][0])\n",
    "                    loss_arr_l.append(-l[0][0])\n",
    "                    if iters is not None and j == iters:\n",
    "                        break\n",
    "                loss_np_arr_l = np.asarray(loss_arr_l)\n",
    "                if j % 100 == 0:\n",
    "                    print('iter: {3}\\t{0} - {1}\\tavg loss: {2}'.format(l1, l2, np.average(loss_np_arr_l), j))\n",
    "                if iters is not None and j == iters:\n",
    "                    break\n",
    "                    \n",
    "            # Monitoring for learning curve\n",
    "            loss_np_arr = np.asarray(loss_arr)\n",
    "            loss_epoch_avg = np.average(loss_np_arr)\n",
    "            print('{0}\\tavg sims: {1}'.format(i, loss_epoch_avg))\n",
    "            lc_arr.append(loss_epoch_avg)\n",
    "            \n",
    "            # Calculate precision\n",
    "            e_prec_l = []\n",
    "            for ((l1, l2), _) in d_wps.items():\n",
    "                m1_tr = copy.deepcopy(d_tr_mods[l1])\n",
    "                m2_tr = copy.deepcopy(d_tr_mods[l2])\n",
    "                m1 = copy.deepcopy(d_models[l1])\n",
    "                m2 = copy.deepcopy(d_models[l2])\n",
    "                # Get translations matrices\n",
    "                idx_l1 = langs.index(l1)\n",
    "                idx_l2 = langs.index(l2)\n",
    "                T1 = T[idx_l1]\n",
    "                T2 = T[idx_l2]\n",
    "                precs_1 = calc_precision(precs_to_calc, m1, m2, d_dict[(l1, l2)], verbose=False)\n",
    "                precs_2 = calc_precision(precs_to_calc, m2, m1, d_dict[(l2, l1)], verbose=False)\n",
    "                # Todo: should be done this way\n",
    "#                 # Prec l1 - l2\n",
    "#                 m1_tr.syn0 = np.dot(m1_tr.syn0, T1)\n",
    "#                 m2.syn0 = np.dot(m2.syn0, T2)\n",
    "#                 precs_1 = calc_precision(precs_to_calc, m1_tr, m2, d_dict[(l1, l2)], verbose=False)\n",
    "#                 # Prec l2 - l1\n",
    "#                 m2_tr.syn0 = np.dot(m2_tr.syn0, T2)\n",
    "#                 m1.syn0 = np.dot(m1.syn0, T1)\n",
    "#                 precs_2 = calc_precision(precs_to_calc, m2_tr, m1, d_dict[(l2, l1)], verbose=False)\n",
    "                e_prec_l.append(((l1, l2), precs_1))\n",
    "                e_prec_l.append(((l2, l1), precs_2))\n",
    "            print(e_prec_l)\n",
    "            precs_arr.append(e_prec_l)\n",
    "    return T, lc_arr, precs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, lc, precs = train(langs, d_models, d_wps, d_tr_mods, 300, 1, [1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
