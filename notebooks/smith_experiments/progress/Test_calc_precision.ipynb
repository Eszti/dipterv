{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing precision calculating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate precision\n",
    "# model_src : source language embeddings (need to have syn0 and index2word properites) (after translation)\n",
    "# model_tar : target language embeddings (need to have syn0 and index2word properites) (can be don in orig or universal space)\n",
    "# dict_scr_2_tar : dictionary from source to target\n",
    "def calc_precision(precs, model_src, model_tar, dict_scr_2_tar):\n",
    "    W_src = model_src.syn0\n",
    "    W_tar = model_tar.syn0\n",
    "    idx_src = model_src.index2word\n",
    "    idx_tar = model_tar.index2word\n",
    "    prec_pcnts = []\n",
    "\n",
    "    cos_mx = cosine_similarity(W_src, W_tar)\n",
    "    print(cos_mx)\n",
    "    sim_mx = np.argsort(-cos_mx, axis=1)\n",
    "    print(sim_mx)\n",
    "    max_prec = max(precs)\n",
    "    prec_cnt = np.zeros(shape=(1, max_prec))\n",
    "    print('word: \\ttranslations in dict: \\tclosest words after translation: \\t')\n",
    "    for i, r in enumerate(sim_mx):\n",
    "        key_word = idx_src[i]\n",
    "        value_words = dict_scr_2_tar[key_word]\n",
    "        closest_words = []\n",
    "        for j in range(max_prec):\n",
    "            word = idx_tar[r[j]]\n",
    "            closest_words.append(word)\n",
    "            if word in value_words:\n",
    "                if j != 0:\n",
    "                    print(word)\n",
    "                    print(value_words)\n",
    "                prec_cnt[0][j] = prec_cnt[0][j] + 1\n",
    "        print('{}\"\\t{}\\t{}'.format(key_word, value_words, closest_words))\n",
    "    print(prec_cnt)\n",
    "    for i, val in enumerate(precs):\n",
    "        sum_hit = np.sum(prec_cnt[0][0:val])\n",
    "        pcnt = float(sum_hit) / sim_mx.shape[0]\n",
    "        print('prec {} : {}'.format(val, pcnt))\n",
    "        prec_pcnts.append(pcnt)\n",
    "    return cos_mx, sim_mx, prec_pcnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_fn = '/mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec'\n",
    "limit = 100\n",
    "precs = [1, 3, 5]\n",
    "model_src = KeyedVectors.load_word2vec_format(emb_fn, binary=False, limit=limit)\n",
    "model_tar = copy.deepcopy(model_src)\n",
    "\n",
    "dict_scr_2_tar = dict()\n",
    "for w in model_src.index2word:\n",
    "    dict_scr_2_tar[w] = [w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000048  0.70641851  0.39606649 ...,  0.2314885   0.26116693\n",
      "   0.23310725]\n",
      " [ 0.70641851  1.00000012  0.40296385 ...,  0.19698849  0.20841369\n",
      "   0.21084428]\n",
      " [ 0.39606649  0.40296385  0.99999964 ...,  0.34884906  0.23713872\n",
      "   0.47506574]\n",
      " ..., \n",
      " [ 0.2314885   0.19698849  0.34884906 ...,  0.99999976  0.20379925\n",
      "   0.31832305]\n",
      " [ 0.26116693  0.20841369  0.23713872 ...,  0.20379925  0.99999994\n",
      "   0.20794965]\n",
      " [ 0.23310725  0.21084428  0.47506574 ...,  0.31832305  0.20794965\n",
      "   1.00000024]]\n",
      "[[ 0  1  9 ..., 70 68 85]\n",
      " [ 1  0 26 ..., 58 71 93]\n",
      " [ 2  4 37 ..., 86 66 56]\n",
      " ..., \n",
      " [97 79  2 ..., 86  3 66]\n",
      " [98 95 25 ..., 43 56 86]\n",
      " [99 65 78 ..., 48 56 86]]\n",
      "word: \ttranslations in dict: \tclosest words after translation: \t\n",
      ",\"\t[',']\t[',', '.', ')', '(', 'and']\n",
      ".\"\t['.']\t['.', ',', 'this', ')', '(']\n",
      "the\"\t['the']\t['the', 'of', 'which', 'and', 'in']\n",
      "</s>\"\t['</s>']\t['</s>', 'utc', '/', 'links', 'can']\n",
      "of\"\t['of']\t['of', 'the', 'and', 'its', 'one']\n",
      "-\"\t['-']\t['-', ',', '.', ')', '(']\n",
      "in\"\t['in']\t['in', 'the', ',', 'this', 'and']\n",
      "and\"\t['and']\t['and', 'the', 'of', 'which', ',']\n",
      "'\"\t[\"'\"]\t[\"'\", 's', '(', ')', 'it']\n",
      ")\"\t[')']\t[')', '(', ',', '.', 'utc']\n",
      "(\"\t['(']\t['(', ')', ',', '.', 'utc']\n",
      "to\"\t['to']\t['to', 'but', 'would', 'this', 'which']\n",
      "a\"\t['a']\t['a', 'the', 'which', 'is', 'in']\n",
      "is\"\t['is']\t['is', 'are', 'it', 'has', 'a']\n",
      "was\"\t['was']\t['was', 'had', 'were', 'been', 'by']\n",
      "on\"\t['on']\t['on', 'the', 'in', 'it', 'at']\n",
      "s\"\t['s']\t['s', \"'\", 'his', 'the', 'as']\n",
      "for\"\t['for']\t['for', 'and', 'the', 'only', 'but']\n",
      "as\"\t['as']\t['as', 'such', 'also', 'the', 'and']\n",
      "by\"\t['by']\t['by', 'was', 'and', 'the', 'which']\n",
      "that\"\t['that']\t['that', 'but', 'which', 'so', 'not']\n",
      "it\"\t['it']\t['it', 'which', 'that', 'but', 'not']\n",
      "with\"\t['with']\t['with', 'and', 'when', 'had', 'other']\n",
      "from\"\t['from']\t['from', 'and', 'of', 'the', 'which']\n",
      "at\"\t['at']\t['at', 'in', 'the', 'time', 'when']\n",
      "he\"\t['he']\t['he', 'she', 'his', 'who', 'her']\n",
      "this\"\t['this']\t['this', 'it', 'that', '.', 'in']\n",
      "be\"\t['be']\t['be', 'should', 'so', 'have', 'not']\n",
      "i\"\t['i']\t['i', 'you', 'if', 'so', 't']\n",
      "an\"\t['an']\t['an', 'the', 'is', 'which', 'it']\n",
      "utc\"\t['utc']\t['utc', 'talk', ')', '(', 'you']\n",
      "his\"\t['his']\t['his', 'her', 'he', 'their', 'she']\n",
      "not\"\t['not']\t['not', 'do', 'but', 'that', 'if']\n",
      "–\"\t['–']\t['–', '(', ')', 'first', 'and']\n",
      "are\"\t['are']\t['are', 'were', 'have', 'other', 'is']\n",
      "or\"\t['or']\t['or', 'such', 'be', 'should', 'if']\n",
      "talk\"\t['talk']\t['talk', 'utc', 'article', 'page', 'should']\n",
      "which\"\t['which']\t['which', 'that', 'the', 'it', 'also']\n",
      "also\"\t['also']\t['also', 'which', 'as', 'and', 'that']\n",
      "has\"\t['has']\t['has', 'been', 'have', 'had', 'is']\n",
      "were\"\t['were']\t['were', 'are', 'had', 'was', 'they']\n",
      "but\"\t['but']\t['but', 'so', 'that', 'when', 'they']\n",
      "have\"\t['have']\t['have', 'been', 'has', 'that', 'they']\n",
      "#\"\t['#']\t['#', '(', ')', '-', '–']\n",
      "one\"\t['one']\t['one', 'two', 'only', 'the', 'other']\n",
      "rd\"\t['rd']\t['rd', 'team', 'th', 'score', 'first']\n",
      "new\"\t['new']\t['new', 'the', 'of', 'in', 'and']\n",
      "first\"\t['first']\t['first', 'one', 'only', 'the', 'after']\n",
      "page\"\t['page']\t['page', 'article', 'should', 'discussion', 'talk']\n",
      "no\"\t['no']\t['no', 'there', 'such', 'should', 'page']\n",
      "you\"\t['you']\t['you', 'i', 'if', 'so', 'do']\n",
      "they\"\t['they']\t['they', 'their', 'but', 'have', 'that']\n",
      "had\"\t['had']\t['had', 'were', 'was', 'have', 'been']\n",
      "article\"\t['article']\t['article', 'page', 'should', 'talk', 'discussion']\n",
      "t\"\t['t']\t['t', 'you', 'i', 'if', 'can']\n",
      "who\"\t['who']\t['who', 'he', 'when', 'that', 'his']\n",
      "?\"\t['?']\t['?', 'you', '.', 'utc', 'talk']\n",
      "all\"\t['all']\t['all', 'only', 'other', 'that', 'and']\n",
      "their\"\t['their']\t['their', 'they', 'his', 'its', 'her']\n",
      "there\"\t['there']\t['there', 'but', 'some', 'that', 'so']\n",
      "been\"\t['been']\t['been', 'has', 'have', 'had', 'be']\n",
      "made\"\t['made']\t['made', 'such', 'should', 'no', 'as']\n",
      "its\"\t['its']\t['its', 'the', 'their', 'which', 'it']\n",
      "people\"\t['people']\t['people', 'there', 'who', 'about', 'they']\n",
      "may\"\t['may']\t['may', 'can', 'would', ',', 'also']\n",
      "after\"\t['after']\t['after', 'when', 'during', 'first', 'time']\n",
      "%\"\t['%']\t['%', '.', '?', '/', 'all']\n",
      "other\"\t['other']\t['other', 'some', 'such', 'and', 'are']\n",
      "should\"\t['should']\t['should', 'be', 'page', 'not', 'article']\n",
      "two\"\t['two']\t['two', 'one', 'other', 'only', 'and']\n",
      "score\"\t['score']\t['score', 'rd', 'team', 'th', 't']\n",
      "her\"\t['her']\t['her', 'she', 'his', 'he', 'their']\n",
      "can\"\t['can']\t['can', 'if', 'so', 'be', 'should']\n",
      "would\"\t['would']\t['would', 'that', 'not', 'but', 'if']\n",
      "more\"\t['more']\t['more', 'so', 'some', 'other', 'one']\n",
      "if\"\t['if']\t['if', 'so', 'but', 'you', 'not']\n",
      "she\"\t['she']\t['she', 'her', 'he', 'his', 'when']\n",
      "about\"\t['about']\t['about', 'there', 'that', 'some', 'but']\n",
      "when\"\t['when']\t['when', 'but', 'after', 'that', 'which']\n",
      "time\"\t['time']\t['time', 'when', 'during', 'years', 'but']\n",
      "team\"\t['team']\t['team', 'rd', 'score', 'first', 'time']\n",
      "american\"\t['american']\t['american', 'united', 'and', 'world', 'who']\n",
      "such\"\t['such']\t['such', 'or', 'as', 'other', 'should']\n",
      "th\"\t['th']\t['th', 'rd', 'first', 'the', 'during']\n",
      "do\"\t['do']\t['do', 'not', 'should', 'you', 'so']\n",
      "discussion\"\t['discussion']\t['discussion', 'page', 'should', 'article', 'talk']\n",
      "links\"\t['links']\t['links', 'this', 'talk', 'page', '.']\n",
      "only\"\t['only']\t['only', 'but', 'one', 'that', 'not']\n",
      "some\"\t['some']\t['some', 'other', 'have', 'but', 'that']\n",
      "up\"\t['up']\t['up', 'and', 'but', 'they', 'when']\n",
      "see\"\t['see']\t['see', 'so', 'if', 'you', 'i']\n",
      "united\"\t['united']\t['united', 'american', 'in', 'the', 'world']\n",
      "years\"\t['years']\t['years', 'time', 'after', 'had', 'during']\n",
      "into\"\t['into']\t['into', 'when', 'which', 'and', 'after']\n",
      "/\"\t['/']\t['/', ')', '(', '-', '</s>']\n",
      "school\"\t['school']\t['school', 'university', 'he', 'at', 'and']\n",
      "so\"\t['so']\t['so', 'but', 'if', 'that', 'you']\n",
      "world\"\t['world']\t['world', 'time', 'the', 'american', 'united']\n",
      "university\"\t['university']\t['university', 'school', 'he', 'and', 'of']\n",
      "during\"\t['during']\t['during', 'after', 'when', 'time', 'first']\n",
      "[[ 100.    0.    0.    0.    0.]]\n",
      "prec 1 : 1.0\n",
      "prec 3 : 1.0\n",
      "prec 5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cos_mx, sim_mx, prec_pcnts = calc_precision(precs=precs, model_src=model_src, model_tar=model_tar, dict_scr_2_tar=dict_scr_2_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  9, ..., 70, 68, 85],\n",
       "       [ 1,  0, 26, ..., 58, 71, 93],\n",
       "       [ 2,  4, 37, ..., 86, 66, 56],\n",
       "       ..., \n",
       "       [97, 79,  2, ..., 86,  3, 66],\n",
       "       [98, 95, 25, ..., 43, 56, 86],\n",
       "       [99, 65, 78, ..., 48, 56, 86]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.39606649,  0.40296385,  0.99999964,  0.14488608,  0.76346207,\n",
       "        0.22284862,  0.59858531,  0.60340977,  0.44609597,  0.33102584,\n",
       "        0.33654031,  0.4662706 ,  0.54423028,  0.47220013,  0.46411642,\n",
       "        0.47597525,  0.47266904,  0.45076308,  0.51366013,  0.40777072,\n",
       "        0.52870935,  0.5673455 ,  0.38784423,  0.42839915,  0.43914062,\n",
       "        0.43046755,  0.48890689,  0.4106057 ,  0.25383443,  0.46081591,\n",
       "        0.18135244,  0.41418207,  0.4516153 ,  0.2697486 ,  0.32298076,\n",
       "        0.43808582,  0.2012071 ,  0.66363746,  0.51235867,  0.34157732,\n",
       "        0.36233664,  0.47308332,  0.33924368,  0.15235579,  0.54275173,\n",
       "        0.17343347,  0.37797648,  0.49200991,  0.20138821,  0.30569857,\n",
       "        0.26338121,  0.43041757,  0.34895226,  0.3434    ,  0.14848886,\n",
       "        0.37043616,  0.09792759,  0.41540202,  0.50073969,  0.39565817,\n",
       "        0.35183194,  0.32243633,  0.59003699,  0.27463552,  0.31303373,\n",
       "        0.43058956,  0.11427952,  0.45316726,  0.30678532,  0.42883623,\n",
       "        0.18453158,  0.34952658,  0.27190852,  0.35708681,  0.33984035,\n",
       "        0.3225036 ,  0.34381548,  0.35307145,  0.49453217,  0.41476026,\n",
       "        0.20888145,  0.30311623,  0.36026204,  0.35088104,  0.30854425,\n",
       "        0.24426077,  0.14416806,  0.5078969 ,  0.43999296,  0.30953148,\n",
       "        0.26044846,  0.33489093,  0.28846541,  0.34202746,  0.1503492 ,\n",
       "        0.23073588,  0.3772254 ,  0.34884906,  0.23713872,  0.47506574], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 2,  4, 37,  7,  6, 62, 21, 12, 44, 20, 18, 38, 87, 58, 78, 47, 26,\n",
       "       15, 99, 41, 16, 13, 11, 14, 29, 67, 32, 17,  8, 88, 24, 35, 65, 25,\n",
       "       51, 69, 23, 57, 79, 31, 27, 19,  1,  0, 59, 22, 46, 96, 55, 40, 82,\n",
       "       73, 77, 60, 83, 71, 52, 97, 76, 53, 93, 39, 74, 42, 10, 91,  9, 34,\n",
       "       75, 61, 64, 89, 84, 68, 49, 81, 92, 63, 72, 33, 50, 90, 28, 85, 98,\n",
       "       95,  5, 80, 48, 36, 70, 30, 45, 43, 94, 54,  3, 86, 66, 56])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-cos_mx, axis=1)\n",
    "cos_mx[2]\n",
    "np.argsort(-cos_mx[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
