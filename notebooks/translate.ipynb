{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from __future__ import print_function\n",
    "from nbformat import current\n",
    "import logging\n",
    "import io, os, time, sys\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "def execute_notebook(nbfile):    \n",
    "    with io.open(nbfile) as f:\n",
    "        nb = current.read(f, 'json')\n",
    "    \n",
    "    ip = get_ipython()\n",
    "    \n",
    "    for cell in nb.worksheets[0].cells:\n",
    "        if cell.cell_type != 'code':\n",
    "            continue\n",
    "        ip.run_cell(cell.input)\n",
    "        \n",
    "execute_notebook(\"functions.ipynb\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found list len: 0\n",
      "[u'all', u'ash', u'bark', u'abdomen', u'big', u'bird', u'bite', u'black', u'blood', u'bone', u'breast', u'burn', u'claw', u'cloud', u'cold', u'come', u'dead', u'dog', u'drink', u'dry', u'ear', u'earth', u'eat', u'egg', u'eye', u'fat', u'feather', u'fire', u'fish', u'fly', u'foot', u'full', u'give', u'good', u'green', u'hair', u'hand', u'head', u'hear', u'heart', u'horn', u'i', u'kill', u'knee', u'know', u'leaf', u'lie', u'liver', u'long', u'louse', u'male', u'many', u'flesh', u'moon', u'hill', u'mouth', u'name', u'neck', u'new', u'night', u'nose', u'not', u'adulterous', u'human', u'rain', u'red', u'path', u'root', u'round', u'sand', u'say', u'see', u'seed', u'sit', u'hide', u'sleep', u'little', u'smoke', u'stand', u'star', u'rock', u'sun', u'swim', u'tail', u'that', u'this', u'thou', u'tongue', u'tooth', u'tree', u'grind', u'go', u'hot', u'water', u'we', u'what', u'white', u'who', u'female', u'yellow', u'distant', u'dull', u'close', u'salt', u'short', u'snake', u'thin', u'wind', u'earthworm', u'year']\n",
      "110\n",
      "Embeddings len: 110\n",
      "Not found: 0\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eszti/.virtualenvs/dipterv/lib/python2.7/site-packages/ipykernel/__main__.py:56: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "num = 110\n",
    "silcodes_fn = '/home/eszti/projects/dipterv/univ_embedding/res/swad_fb_{}.json'.format(num)\n",
    "\n",
    "with open(silcodes_fn) as f:\n",
    "    silcodes = json.load(f)\n",
    "    \n",
    "sil2fbcodes_fn = '/home/eszti/projects/dipterv/univ_embedding/res/sil2fbcodes.json'\n",
    "with open(sil2fbcodes_fn) as f:\n",
    "    sil2fb = json.load(f)\n",
    "\n",
    "swad_idx = []\n",
    "en_swad_fn = '/home/eszti/data/panlex_swadesh/swadesh{}/eng-000.txt'.format(num)\n",
    "en_embed_fn = '/mnt/permanent/Language/Multi/FB/wiki.en/wiki.en.vec'\n",
    "en_swad, en_emb, en_nfi = get_embedding(en_swad_fn, swad_idx, en_embed_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swad file: /home/eszti/data/panlex_swadesh/swadesh110/tat-000.txt\n",
      "embedding file: /mnt/permanent/Language/Multi/FB/wiki.tt/wiki.tt.vec\n",
      "Not found list len: 6\n",
      "[u'\\u0431\\u0430\\u0440\\u044b', u'\\u043a\\u04e9\\u043b', u'\\u043a\\u0430\\u0431\\u044b\\u043a', u'\\u043a\\u043e\\u0440\\u0441\\u0430\\u043a', u'\\u0437\\u0443\\u0440', u'\\u043a\\u043e\\u0448', u'\\u0442\\u0435\\u0448\\u043b\\u04d9\\u0440\\u0433\\u04d9', u'\\u043a\\u0430\\u0440\\u0430', u'kan', u'seyak', u'\\u0438\\u043c\\u0447\\u04d9\\u043a', u'\\u0431\\u043e\\u043b\\u044b\\u0442', u'\\u0441\\u0430\\u043b\\u043a\\u044b\\u043d', u'kil', u'il', u'et', u'ec', u'\\u043a\\u043e\\u0440\\u044b', u'kolak', u'\\u0442\\u0443\\u0444\\u0440\\u0430\\u043a', u'\\u0430\\u0448\\u0430\\u0440\\u0433\\u0430', u'\\u0439\\u043e\\u043c\\u044b\\u0440\\u043a\\u0430', u'kiz', u'\\u043a\\u0430\\u0443\\u0440\\u044b\\u0439', u'ut', u'balok', u'\\u043e\\u0447\\u0430\\u0440\\u0433\\u0430', u'\\u0430\\u044f\\u043a', u'tulo', u'\\u0431\\u0438\\u0440\\u0435\\u0440\\u0433\\u04d9', u'\\u044f\\u0445\\u0448\\u044b', u'\\u044f\\u0448\\u0435\\u043b', u'\\u043a\\u044b\\u043b', u'kul', u'\\u0431\\u0430\\u0448', u'tonl', u'\\u0439\\u04e9\\u0440\\u04d9\\u043a', u'megez', u'min', u'\\u04af\\u0442\\u0435\\u0440\\u0435\\u0440\\u0433\\u04d9', u'tez', u'\\u0431\\u0435\\u043b\\u0435\\u0440\\u0433\\u04d9', u'yaprak', u'bawor', u'\\u043e\\u0437\\u044b\\u043d', u'\\u0431\\u0435\\u0442', u'\\u0438\\u0440', u'\\u043a\\u04af\\u043f', u'\\u0430\\u0439', u'\\u0442\\u0430\\u0443', u'\\u0430\\u0432\\u044b\\u0437', u'\\u0430\\u0442', u'\\u043c\\u0443\\u0435\\u043d', u'yana', u'ten', u'boron', u'\\u0442\\u04af\\u0433\\u0435\\u043b', u'ber', u'adem', u'\\u044f\\u04a3\\u0433\\u044b\\u0440', u'\\u043a\\u044b\\u0437\\u044b\\u043b', u'yul', u'\\u0442\\u0430\\u043c\\u044b\\u0440', u'\\u0439\\u043e\\u043c\\u0440\\u044b', u'\\u043a\\u043e\\u043c', u'\\u04d9\\u0439\\u0442\\u0435\\u0440\\u0433\\u04d9', u'kir', u'\\u0431\\u04e9\\u0440\\u0442\\u0435\\u043a', u'\\u0443\\u0442\\u044b\\u0440\\u044b\\u0440\\u0433\\u0430', u'kin', u'\\u0439\\u043e\\u043a\\u043b\\u0430\\u0440\\u0433\\u0430', u'\\u043a\\u0435\\u0447\\u0435', u'\\u0442\\u04e9\\u0442\\u0435\\u043d', u'yoldoz', u'tas', u'koyas', u'\\u0439\\u04e9\\u0437\\u04d9\\u0440\\u0433\\u04d9', u'\\u043a\\u043e\\u0439\\u0440\\u044b\\u043a', u'\\u0442\\u0435\\u0433\\u0435', u'\\u0431\\u0443', u'sin', u'tel', u'tes', u'\\u0430\\u0433\\u0430\\u0447', u'ike', u'\\u0430\\u0442\\u043b\\u0430\\u0443', u'\\u0497\\u044b\\u043b\\u044b', u'su', u'bez', u'\\u043d\\u0438', u'\\u0430\\u043a', u'\\u043a\\u0435\\u043c', u'\\u0445\\u0430\\u0442\\u044b\\u043d', u'\\u0441\\u0430\\u0440\\u044b', u'\\u0435\\u0440\\u0430\\u043a', u'\\u0430\\u0432\\u044b\\u0440', u'\\u044f\\u043a\\u044b\\u043d', u'\\u0442\\u043e\\u0437', u'\\u043a\\u044b\\u0441\\u043a\\u0430', u'\\u0435\\u043b\\u0430\\u043d', u'\\u043d\\u0435\\u0447\\u043a\\u04d9', u'\\u0497\\u0438\\u043b', u'\\u043a\\u043e\\u0440\\u0442', u'\\u0435\\u043b']\n",
      "104\n",
      "Embeddings len: 86\n",
      "Not found: 24\n",
      "[ 6  9 10 11 12 20 24 25 28 31 38 40 43 45 46 47 52 60 63 74 78 79 80 81]\n",
      "86\n",
      "86\n",
      "[ 6  9 10 11 12 20 24 25 28 31 38 40 43 45 46 47 52 60 63 74 78 79 80 81]\n",
      "EMBED:\n",
      "[[-0.1168033   0.01926405 -0.04619401 ...,  0.0540062  -0.07060581\n",
      "   0.09131702]\n",
      " [ 0.08036612 -0.06442716 -0.09113833 ...,  0.03141265  0.05199991\n",
      "   0.06838642]\n",
      " [ 0.05669455 -0.02427909 -0.03126268 ...,  0.06732765 -0.01482688\n",
      "  -0.05914922]\n",
      " ..., \n",
      " [-0.00161818 -0.04880033 -0.08882938 ...,  0.06453764  0.0124443\n",
      "   0.04486193]\n",
      " [ 0.02328351  0.01894428  0.0314911  ..., -0.0780181   0.01700877\n",
      "   0.09182414]\n",
      " [ 0.05432583 -0.05331619 -0.08558351 ...,  0.04302122 -0.08384334\n",
      "   0.02386425]]\n",
      "86\n",
      "86\n",
      "Initialized\n",
      "Loss at step 0: 343.809631\n",
      "Loss at step 100: 336.437683\n",
      "Loss at step 200: 329.507202\n",
      "Loss at step 300: 322.970886\n",
      "Loss at step 400: 316.782379\n",
      "Loss at step 500: 310.897705\n",
      "Loss at step 600: 305.276550\n",
      "Loss at step 700: 299.882080\n",
      "Loss at step 800: 294.681763\n",
      "Loss at step 900: 289.647278\n",
      "Loss at step 1000: 284.754120\n",
      "Loss at step 1100: 279.981323\n",
      "Loss at step 1200: 275.311554\n",
      "Loss at step 1300: 270.730194\n",
      "Loss at step 1400: 266.225342\n",
      "Loss at step 1500: 261.786987\n",
      "Loss at step 1600: 257.407227\n",
      "Loss at step 1700: 253.079895\n",
      "Loss at step 1800: 248.799469\n",
      "Loss at step 1900: 244.562164\n",
      "Loss at step 2000: 240.364777\n",
      "Loss at step 2100: 236.204865\n",
      "Loss at step 2200: 232.080460\n",
      "Loss at step 2300: 227.990295\n",
      "Loss at step 2400: 223.933228\n",
      "Loss at step 2500: 219.908630\n",
      "Loss at step 2600: 215.916229\n",
      "Loss at step 2700: 211.955688\n",
      "Loss at step 2800: 208.027191\n",
      "Loss at step 2900: 204.130707\n",
      "Loss at step 3000: 200.266891\n",
      "Loss at step 3100: 196.435791\n",
      "Loss at step 3200: 192.638123\n",
      "Loss at step 3300: 188.874817\n",
      "Loss at step 3400: 185.146179\n",
      "Loss at step 3500: 181.453354\n",
      "Loss at step 3600: 177.797134\n",
      "Loss at step 3700: 174.178604\n",
      "Loss at step 3800: 170.598846\n",
      "Loss at step 3900: 167.058868\n",
      "Loss at step 4000: 163.560043\n",
      "Loss at step 4100: 160.103516\n",
      "Loss at step 4200: 156.690643\n",
      "Loss at step 4300: 153.322876\n",
      "Loss at step 4400: 150.001617\n",
      "Loss at step 4500: 146.728333\n",
      "Loss at step 4600: 143.504593\n",
      "Loss at step 4700: 140.331879\n",
      "Loss at step 4800: 137.211853\n",
      "Loss at step 4900: 134.146088\n",
      "Loss at step 5000: 131.136154\n",
      "Loss at step 5100: 128.183777\n",
      "Loss at step 5200: 125.290367\n",
      "Loss at step 5300: 122.457542\n",
      "Loss at step 5400: 119.686913\n",
      "Loss at step 5500: 116.979782\n",
      "Loss at step 5600: 114.337723\n",
      "Loss at step 5700: 111.761856\n",
      "Loss at step 5800: 109.253433\n",
      "Loss at step 5900: 106.813568\n",
      "Loss at step 6000: 104.443024\n",
      "Loss at step 6100: 102.142670\n",
      "Loss at step 6200: 99.913048\n",
      "Loss at step 6300: 97.754456\n",
      "Loss at step 6400: 95.667206\n",
      "Loss at step 6500: 93.651123\n",
      "Loss at step 6600: 91.705948\n",
      "Loss at step 6700: 89.831223\n",
      "Loss at step 6800: 88.026070\n",
      "Loss at step 6900: 86.289673\n",
      "Loss at step 7000: 84.620636\n",
      "Loss at step 7100: 83.017654\n",
      "Loss at step 7200: 81.479095\n",
      "Loss at step 7300: 80.003044\n",
      "Loss at step 7400: 78.587555\n",
      "Loss at step 7500: 77.230431\n",
      "Loss at step 7600: 75.929443\n",
      "Loss at step 7700: 74.682144\n",
      "Loss at step 7800: 73.486122\n",
      "Loss at step 7900: 72.338852\n",
      "Loss at step 8000: 71.237801\n",
      "Loss at step 8100: 70.180389\n",
      "Loss at step 8200: 69.164139\n",
      "Loss at step 8300: 68.186554\n",
      "Loss at step 8400: 67.245140\n",
      "Loss at step 8500: 66.337601\n",
      "Loss at step 8600: 65.461639\n",
      "Loss at step 8700: 64.615067\n",
      "Loss at step 8800: 63.795845\n",
      "Loss at step 8900: 63.001942\n",
      "Loss at step 9000: 62.231522\n",
      "Loss at step 9100: 61.482826\n",
      "Loss at step 9200: 60.754257\n",
      "Loss at step 9300: 60.044285\n",
      "Loss at step 9400: 59.351536\n",
      "Loss at step 9500: 58.674644\n",
      "Loss at step 9600: 58.012451\n",
      "Loss at step 9700: 57.363926\n",
      "Loss at step 9800: 56.728012\n",
      "Loss at step 9900: 56.103779\n",
      "Loss at step 10000: 55.490482\n",
      "Loss at step 10100: 54.887306\n",
      "Loss at step 10200: 54.293648\n",
      "Loss at step 10300: 53.708813\n",
      "Loss at step 10400: 53.132347\n",
      "Loss at step 10500: 52.563744\n",
      "Loss at step 10600: 52.002560\n",
      "Loss at step 10700: 51.448395\n",
      "Loss at step 10800: 50.900906\n",
      "Loss at step 10900: 50.359810\n",
      "Loss at step 11000: 49.824799\n",
      "Loss at step 11100: 49.295658\n",
      "Loss at step 11200: 48.772194\n",
      "Loss at step 11300: 48.254177\n",
      "Loss at step 11400: 47.741455\n",
      "Loss at step 11500: 47.233898\n",
      "Loss at step 11600: 46.731339\n",
      "Loss at step 11700: 46.233715\n",
      "Loss at step 11800: 45.740868\n",
      "Loss at step 11900: 45.252701\n",
      "Loss at step 12000: 44.769218\n",
      "Loss at step 12100: 44.290245\n",
      "Loss at step 12200: 43.815807\n",
      "Loss at step 12300: 43.345764\n",
      "Loss at step 12400: 42.880085\n",
      "Loss at step 12500: 42.418697\n",
      "Loss at step 12600: 41.961628\n",
      "Loss at step 12700: 41.508743\n",
      "Loss at step 12800: 41.060036\n",
      "Loss at step 12900: 40.615456\n",
      "Loss at step 13000: 40.265068\n",
      "Loss at step 13100: 39.946724\n",
      "Loss at step 13200: 39.629673\n",
      "Loss at step 13300: 39.313828\n",
      "Loss at step 13400: 38.999203\n",
      "Loss at step 13500: 38.685768\n",
      "Loss at step 13600: 38.373596\n",
      "Loss at step 13700: 38.062580\n",
      "Loss at step 13800: 37.752823\n",
      "Loss at step 13900: 37.444324\n",
      "Loss at step 14000: 37.137005\n",
      "Loss at step 14100: 36.830978\n",
      "Loss at step 14200: 36.526154\n",
      "Loss at step 14300: 36.222584\n",
      "Loss at step 14400: 35.920280\n",
      "Loss at step 14500: 35.619232\n",
      "Loss at step 14600: 35.319450\n",
      "Loss at step 14700: 35.020912\n",
      "Loss at step 14800: 34.723640\n",
      "Loss at step 14900: 34.427673\n",
      "Loss at step 15000: 34.132957\n",
      "Loss at step 15100: 33.839539\n",
      "Loss at step 15200: 33.547413\n",
      "Loss at step 15300: 33.256550\n",
      "Loss at step 15400: 32.967018\n",
      "Loss at step 15500: 32.678761\n",
      "Loss at step 15600: 32.391846\n",
      "Loss at step 15700: 32.106224\n",
      "Loss at step 15800: 31.821894\n",
      "Loss at step 15900: 31.538891\n",
      "Loss at step 16000: 31.257202\n",
      "Loss at step 16100: 30.976864\n",
      "Loss at step 16200: 30.697847\n",
      "Loss at step 16300: 30.420177\n",
      "Loss at step 16400: 30.143814\n",
      "Loss at step 16500: 29.868793\n",
      "Loss at step 16600: 29.595137\n",
      "Loss at step 16700: 29.322840\n",
      "Loss at step 16800: 29.051870\n",
      "Loss at step 16900: 28.782261\n",
      "Loss at step 17000: 28.514019\n",
      "Loss at step 17100: 28.247139\n",
      "Loss at step 17200: 27.981640\n",
      "Loss at step 17300: 27.717505\n",
      "Loss at step 17400: 27.454739\n",
      "Loss at step 17500: 27.193356\n",
      "Loss at step 17600: 26.933340\n",
      "Loss at step 17700: 26.674696\n",
      "Loss at step 17800: 26.417488\n",
      "Loss at step 17900: 26.161629\n",
      "Loss at step 18000: 25.907154\n",
      "Loss at step 18100: 25.654112\n",
      "Loss at step 18200: 25.402468\n",
      "Loss at step 18300: 25.152182\n",
      "Loss at step 18400: 24.903320\n",
      "Loss at step 18500: 24.655867\n",
      "Loss at step 18600: 24.409822\n",
      "Loss at step 18700: 24.165195\n",
      "Loss at step 18800: 23.921963\n",
      "Loss at step 18900: 23.680151\n",
      "Loss at step 19000: 23.439747\n",
      "Loss at step 19100: 23.200768\n",
      "Loss at step 19200: 22.963203\n",
      "Loss at step 19300: 22.727068\n",
      "Loss at step 19400: 22.492348\n",
      "Loss at step 19500: 22.259047\n",
      "Loss at step 19600: 22.027163\n",
      "Loss at step 19700: 21.796711\n",
      "Loss at step 19800: 21.567675\n",
      "Loss at step 19900: 21.340067\n",
      "Loss at step 20000: 21.113895\n",
      "Loss at step 20100: 20.889128\n",
      "Loss at step 20200: 20.665813\n",
      "Loss at step 20300: 20.443911\n",
      "Loss at step 20400: 20.223434\n",
      "Loss at step 20500: 20.004395\n",
      "Loss at step 20600: 19.786766\n",
      "Loss at step 20700: 19.570566\n",
      "Loss at step 20800: 19.355791\n",
      "Loss at step 20900: 19.142439\n",
      "Loss at step 21000: 18.930513\n",
      "Loss at step 21100: 18.720001\n",
      "Loss at step 21200: 18.510912\n",
      "Loss at step 21300: 18.303236\n",
      "Loss at step 21400: 18.096981\n",
      "Loss at step 21500: 17.892128\n",
      "Loss at step 21600: 17.688705\n",
      "Loss at step 21700: 17.486668\n",
      "Loss at step 21800: 17.286074\n",
      "Loss at step 21900: 17.086855\n",
      "Loss at step 22000: 16.889061\n",
      "Loss at step 22100: 16.692646\n",
      "Loss at step 22200: 16.497648\n",
      "Loss at step 22300: 16.304020\n",
      "Loss at step 22400: 16.111794\n",
      "Loss at step 22500: 15.920961\n",
      "Loss at step 22600: 15.731493\n",
      "Loss at step 22700: 15.543422\n",
      "Loss at step 22800: 15.356721\n",
      "Loss at step 22900: 15.171386\n",
      "Loss at step 23000: 14.987425\n",
      "Loss at step 23100: 14.804818\n",
      "Loss at step 23200: 14.623582\n",
      "Loss at step 23300: 14.443694\n",
      "Loss at step 23400: 14.265145\n",
      "Loss at step 23500: 14.087934\n",
      "Loss at step 23600: 13.912082\n",
      "Loss at step 23700: 13.737553\n",
      "Loss at step 23800: 13.564352\n",
      "Loss at step 23900: 13.392475\n",
      "Loss at step 24000: 13.221916\n",
      "Loss at step 24100: 13.052662\n",
      "Loss at step 24200: 12.884720\n",
      "Loss at step 24300: 12.718074\n",
      "Loss at step 24400: 12.552724\n",
      "Loss at step 24500: 12.388657\n",
      "Loss at step 24600: 12.225883\n",
      "Loss at step 24700: 12.064366\n",
      "Loss at step 24800: 11.904123\n",
      "Loss at step 24900: 11.745160\n",
      "Loss at step 25000: 11.587427\n",
      "Loss at step 25100: 11.430951\n",
      "Loss at step 25200: 11.275717\n",
      "Loss at step 25300: 11.121720\n",
      "Loss at step 25400: 10.968954\n",
      "Loss at step 25500: 10.817408\n",
      "Loss at step 25600: 10.667075\n",
      "Loss at step 25700: 10.517956\n",
      "Loss at step 25800: 10.370029\n",
      "Loss at step 25900: 10.223297\n",
      "Loss at step 26000: 10.077746\n",
      "Loss at step 26100: 9.933387\n",
      "Loss at step 26200: 9.790183\n",
      "Loss at step 26300: 9.648151\n",
      "Loss at step 26400: 9.507281\n",
      "Loss at step 26500: 9.367565\n",
      "Loss at step 26600: 9.228981\n",
      "Loss at step 26700: 9.091545\n",
      "Loss at step 26800: 8.955234\n",
      "Loss at step 26900: 8.820037\n",
      "Loss at step 27000: 8.685963\n",
      "Loss at step 27100: 8.552995\n",
      "Loss at step 27200: 8.421123\n",
      "Loss at step 27300: 8.290351\n",
      "Loss at step 27400: 8.160660\n",
      "Loss at step 27500: 8.032056\n",
      "Loss at step 27600: 7.904515\n",
      "Loss at step 27700: 7.778045\n",
      "Loss at step 27800: 7.652627\n",
      "Loss at step 27900: 7.528273\n",
      "Loss at step 28000: 7.404953\n",
      "Loss at step 28100: 7.282673\n",
      "Loss at step 28200: 7.161421\n",
      "Loss at step 28300: 7.041202\n",
      "Loss at step 28400: 6.921992\n",
      "Loss at step 28500: 6.803796\n",
      "Loss at step 28600: 6.686609\n",
      "Loss at step 28700: 6.570419\n",
      "Loss at step 28800: 6.455215\n",
      "Loss at step 28900: 6.341007\n",
      "Loss at step 29000: 6.227773\n",
      "Loss at step 29100: 6.115509\n",
      "Loss at step 29200: 6.004219\n",
      "Loss at step 29300: 5.893886\n",
      "Loss at step 29400: 5.784511\n",
      "Loss at step 29500: 5.676087\n",
      "Loss at step 29600: 5.568602\n",
      "Loss at step 29700: 5.462058\n",
      "Loss at step 29800: 5.356447\n",
      "Loss at step 29900: 5.251760\n",
      "Loss at step 30000: 5.147999\n",
      "Loss at step 30100: 5.045149\n",
      "Loss at step 30200: 4.943214\n",
      "Loss at step 30300: 4.842180\n",
      "Loss at step 30400: 4.742051\n",
      "Loss at step 30500: 4.642813\n",
      "Loss at step 30600: 4.544467\n",
      "Loss at step 30700: 4.447009\n",
      "Loss at step 30800: 4.350429\n",
      "Loss at step 30900: 4.254725\n",
      "Loss at step 31000: 4.159893\n",
      "Loss at step 31100: 4.065927\n",
      "Loss at step 31200: 3.972821\n",
      "Loss at step 31300: 3.880575\n",
      "Loss at step 31400: 3.789180\n",
      "Loss at step 31500: 3.698634\n",
      "Loss at step 31600: 3.608931\n",
      "Loss at step 31700: 3.520071\n",
      "Loss at step 31800: 3.432043\n",
      "Loss at step 31900: 3.344850\n",
      "Loss at step 32000: 3.258484\n",
      "Loss at step 32100: 3.172941\n",
      "Loss at step 32200: 3.088219\n",
      "Loss at step 32300: 3.004312\n",
      "Loss at step 32400: 2.921220\n",
      "Loss at step 32500: 2.838934\n",
      "Loss at step 32600: 2.757455\n",
      "Loss at step 32700: 2.676776\n",
      "Loss at step 32800: 2.596894\n",
      "Loss at step 32900: 2.517808\n",
      "Loss at step 33000: 2.439513\n",
      "Loss at step 33100: 2.362008\n",
      "Loss at step 33200: 2.285284\n",
      "Loss at step 33300: 2.209343\n",
      "Loss at step 33400: 2.134179\n",
      "Loss at step 33500: 2.059791\n",
      "Loss at step 33600: 1.986176\n",
      "Loss at step 33700: 1.913326\n",
      "Loss at step 33800: 1.841246\n",
      "Loss at step 33900: 1.769926\n",
      "Loss at step 34000: 1.699368\n",
      "Loss at step 34100: 1.629567\n",
      "Loss at step 34200: 1.560520\n",
      "Loss at step 34300: 1.492224\n",
      "Loss at step 34400: 1.424679\n",
      "Loss at step 34500: 1.357879\n",
      "Loss at step 34600: 1.291826\n",
      "Loss at step 34700: 1.226511\n",
      "Loss at step 34800: 1.161938\n",
      "Loss at step 34900: 1.098103\n",
      "Loss at step 35000: 1.035002\n",
      "Loss at step 35100: 0.972636\n",
      "Loss at step 35200: 0.911000\n",
      "Loss at step 35300: 0.850097\n",
      "Loss at step 35400: 0.789922\n",
      "Loss at step 35500: 0.730478\n",
      "Loss at step 35600: 0.671760\n",
      "Loss at step 35700: 0.613773\n",
      "Loss at step 35800: 0.556515\n",
      "Loss at step 35900: 0.499989\n",
      "Loss at step 36000: 0.444198\n",
      "Loss at step 36100: 0.389147\n",
      "Loss at step 36200: 0.334844\n",
      "Loss at step 36300: 0.281300\n",
      "Loss at step 36400: 0.228538\n",
      "Loss at step 36500: 0.176623\n",
      "Loss at step 36600: 0.127951\n",
      "Loss at step 36700: 0.127605\n",
      "Loss at step 36800: 0.127716\n",
      "Loss at step 36900: 0.127748\n",
      "Loss at step 37000: 0.127760\n",
      "Loss at step 37100: 0.127765\n",
      "Loss at step 37200: 0.127768\n",
      "Loss at step 37300: 0.127769\n",
      "Loss at step 37400: 0.127769\n",
      "Loss at step 37500: 0.127772\n",
      "Loss at step 37600: 0.127768\n",
      "Loss at step 37700: 0.127768\n",
      "Loss at step 37800: 0.127767\n",
      "Loss at step 37900: 0.127766\n",
      "Loss at step 38000: 0.127765\n",
      "Loss at step 38100: 0.127765\n",
      "Loss at step 38200: 0.127767\n",
      "Loss at step 38300: 0.127767\n",
      "Loss at step 38400: 0.127764\n",
      "Loss at step 38500: 0.127765\n",
      "Loss at step 38600: 0.127764\n",
      "Loss at step 38700: 0.127766\n",
      "Loss at step 38800: 0.127768\n",
      "Loss at step 38900: 0.127766\n",
      "Loss at step 39000: 0.127767\n",
      "Loss at step 39100: 0.127766\n",
      "Loss at step 39200: 0.127765\n",
      "Loss at step 39300: 0.127764\n",
      "Loss at step 39400: 0.127765\n",
      "Loss at step 39500: 0.127766\n",
      "Loss at step 39600: 0.127766\n",
      "Loss at step 39700: 0.127765\n",
      "Loss at step 39800: 0.127766\n",
      "Loss at step 39900: 0.127767\n",
      "Loss at step 40000: 0.127764\n",
      "Loss at step 40100: 0.127764\n",
      "Loss at step 40200: 0.127764\n",
      "Loss at step 40300: 0.127763\n",
      "Loss at step 40400: 0.127763\n",
      "Loss at step 40500: 0.127763\n",
      "Loss at step 40600: 0.127762\n",
      "Loss at step 40700: 0.127763\n",
      "Loss at step 40800: 0.127763\n",
      "Loss at step 40900: 0.127764\n",
      "Loss at step 41000: 0.127764\n",
      "Loss at step 41100: 0.127765\n",
      "Loss at step 41200: 0.127763\n",
      "Loss at step 41300: 0.127762\n",
      "Loss at step 41400: 0.127760\n",
      "Loss at step 41500: 0.127759\n",
      "Loss at step 41600: 0.127758\n",
      "Loss at step 41700: 0.127757\n",
      "Loss at step 41800: 0.127758\n",
      "Loss at step 41900: 0.127759\n",
      "Loss at step 42000: 0.127758\n",
      "Loss at step 42100: 0.127758\n",
      "Loss at step 42200: 0.127759\n",
      "Loss at step 42300: 0.127759\n",
      "Loss at step 42400: 0.127759\n",
      "Loss at step 42500: 0.127759\n",
      "Loss at step 42600: 0.127757\n",
      "Loss at step 42700: 0.127758\n",
      "Loss at step 42800: 0.127758\n",
      "Loss at step 42900: 0.127758\n",
      "Loss at step 43000: 0.127756\n",
      "Loss at step 43100: 0.127756\n",
      "Loss at step 43200: 0.127757\n",
      "Loss at step 43300: 0.127756\n",
      "Loss at step 43400: 0.127754\n",
      "Loss at step 43500: 0.127756\n",
      "Loss at step 43600: 0.127755\n",
      "Loss at step 43700: 0.127756\n",
      "Loss at step 43800: 0.127754\n",
      "Loss at step 43900: 0.127755\n",
      "Loss at step 44000: 0.127756\n",
      "Loss at step 44100: 0.127755\n",
      "Loss at step 44200: 0.127757\n",
      "Loss at step 44300: 0.127756\n",
      "Loss at step 44400: 0.127756\n",
      "Loss at step 44500: 0.127755\n",
      "Loss at step 44600: 0.127755\n",
      "Loss at step 44700: 0.127757\n",
      "Loss at step 44800: 0.127757\n",
      "Loss at step 44900: 0.127758\n",
      "Loss at step 45000: 0.127758\n",
      "Loss at step 45100: 0.127758\n",
      "Loss at step 45200: 0.127759\n",
      "Loss at step 45300: 0.127759\n",
      "Loss at step 45400: 0.127758\n",
      "Loss at step 45500: 0.127759\n",
      "Loss at step 45600: 0.127760\n",
      "Loss at step 45700: 0.127760\n",
      "Loss at step 45800: 0.127762\n",
      "Loss at step 45900: 0.127762\n",
      "Loss at step 46000: 0.127763\n",
      "Loss at step 46100: 0.127760\n",
      "Loss at step 46200: 0.127759\n",
      "Loss at step 46300: 0.127758\n",
      "Loss at step 46400: 0.127756\n",
      "Loss at step 46500: 0.127755\n",
      "Loss at step 46600: 0.127755\n",
      "Loss at step 46700: 0.127754\n",
      "Loss at step 46800: 0.127756\n",
      "Loss at step 46900: 0.127756\n",
      "Loss at step 47000: 0.127755\n",
      "Loss at step 47100: 0.127754\n",
      "Loss at step 47200: 0.127753\n",
      "Loss at step 47300: 0.127752\n",
      "Loss at step 47400: 0.127752\n",
      "Loss at step 47500: 0.127751\n",
      "Loss at step 47600: 0.127750\n",
      "Loss at step 47700: 0.127750\n",
      "Loss at step 47800: 0.127750\n",
      "Loss at step 47900: 0.127750\n",
      "Loss at step 48000: 0.127749\n",
      "Loss at step 48100: 0.127748\n",
      "Loss at step 48200: 0.127748\n",
      "Loss at step 48300: 0.127749\n",
      "Loss at step 48400: 0.127749\n",
      "Loss at step 48500: 0.127750\n",
      "Loss at step 48600: 0.127749\n",
      "Loss at step 48700: 0.127750\n",
      "Loss at step 48800: 0.127748\n",
      "Loss at step 48900: 0.127747\n",
      "Loss at step 49000: 0.127748\n",
      "Loss at step 49100: 0.127748\n",
      "Loss at step 49200: 0.127746\n",
      "Loss at step 49300: 0.127745\n",
      "Loss at step 49400: 0.127745\n",
      "Loss at step 49500: 0.127745\n",
      "Loss at step 49600: 0.127746\n",
      "Loss at step 49700: 0.127746\n",
      "Loss at step 49800: 0.127747\n",
      "Loss at step 49900: 0.127747\n",
      "\n",
      "\n",
      "Transform 1:\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Transform 2:\n",
      "[[-0.97387707 -0.16540432  0.70374584 ..., -1.26793897 -0.11782158\n",
      "  -0.49251971]\n",
      " [-0.44059327  1.34098887  0.5500896  ..., -0.08549691  1.28653419\n",
      "   0.63385558]\n",
      " [ 0.08941352 -0.35191262  0.40238312 ...,  0.66095096  0.17084904\n",
      "  -0.06879809]\n",
      " ..., \n",
      " [-0.57448781  0.4705604  -1.36755478 ..., -0.58443439  0.14066434\n",
      "  -0.45419198]\n",
      " [-0.21243024 -0.76752901 -0.44068846 ..., -0.59384543  0.04800531\n",
      "   1.30716968]\n",
      " [-0.13972206  1.50552666  0.06783601 ..., -0.36179489 -1.74106586\n",
      "  -1.56086409]]\n",
      "Universal embedding:\n",
      "[[-0.16263153  0.00338254 -0.07691444 ..., -0.06643566  0.04979067\n",
      "  -0.01089721]\n",
      " [-0.00487836 -0.04110851  0.0121155  ...,  0.00265321  0.01838283\n",
      "   0.10599192]\n",
      " [-0.08155832 -0.02407477 -0.03502516 ...,  0.01922793  0.04067671\n",
      "   0.05568752]\n",
      " ..., \n",
      " [-0.12868407 -0.00621222 -0.02864493 ...,  0.03847063  0.13024126\n",
      "   0.02866594]\n",
      " [ 0.00757052 -0.06634758 -0.06222321 ...,  0.11622562 -0.09502848\n",
      "  -0.02430841]\n",
      " [-0.03291285 -0.0584868   0.08132944 ..., -0.03098869  0.03396387\n",
      "  -0.01346373]]\n",
      "\n",
      "\n",
      "W1*T1:\n",
      "[[-0.16262536  0.00336121 -0.07696553 ..., -0.06643926  0.0498531\n",
      "  -0.0108726 ]\n",
      " [-0.00487061 -0.04113529  0.01205136 ...,  0.00264871  0.01846122\n",
      "   0.10602283]\n",
      " [-0.08154867 -0.0241081  -0.03510501 ...,  0.01922233  0.04077433\n",
      "   0.05572599]\n",
      " ..., \n",
      " [-0.12867607 -0.00623986 -0.02871119 ...,  0.03846597  0.13032223\n",
      "   0.02869786]\n",
      " [ 0.00757869 -0.06637575 -0.06229071 ...,  0.11622088 -0.09494597\n",
      "  -0.0242759 ]\n",
      " [-0.03290679 -0.05850771  0.08127929 ..., -0.03099219  0.03402516\n",
      "  -0.01343957]]\n",
      "W2*T2:\n",
      "[[-0.16255872  0.00313157 -0.0775159  ..., -0.06647791  0.05052578\n",
      "  -0.01060745]\n",
      " [-0.00478694 -0.04142365  0.01136036 ...,  0.00260015  0.01930583\n",
      "   0.10635568]\n",
      " [-0.08144464 -0.024467   -0.03596545 ...,  0.01916194  0.04182635\n",
      "   0.05614066]\n",
      " ..., \n",
      " [-0.12858972 -0.0065378  -0.02942518 ...,  0.03841591  0.13119479\n",
      "   0.02904189]\n",
      " [ 0.00766664 -0.06667927 -0.06301813 ...,  0.11616972 -0.09405673\n",
      "  -0.02392545]\n",
      " [-0.03284148 -0.05873306  0.080739   ..., -0.03103002  0.03468547\n",
      "  -0.01317924]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (24,) and (300,) not aligned: 24 (dim 0) != 300 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c448ca3dd78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Calculate missing embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0men_emb_mis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0memb_mis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_emb_mis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Modify embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (24,) and (300,) not aligned: 24 (dim 0) != 300 (dim 0)"
     ]
    }
   ],
   "source": [
    "main_folder = '/home/eszti/data/embeddings/fb_trans/'\n",
    "time_str = time.strftime(\"%H%M\")\n",
    "date_str = time.strftime(\"%Y%m%d\")\n",
    "trans_dir = os.path.join(main_folder, 'trans', '{0}_{1}'.format(date_str, time_str))\n",
    "embed_dir = os.path.join(main_folder, 'embedding', '{0}_{1}'.format(date_str, time_str))\n",
    "\n",
    "os.makedirs(trans_dir)\n",
    "os.makedirs(embed_dir)\n",
    "\n",
    "logging.info('making directory for translation matrices: {}'.format(trans_dir))\n",
    "logging.info('making directory for embeddings: {}'.format(embed_dir))\n",
    "\n",
    "for sil in silcodes:   \n",
    "    if sil == 'eng':\n",
    "        continue\n",
    "    logging.info('Translating {} language...'.format(sil))\n",
    "    swad_fn = '/home/eszti/data/panlex_swadesh/swadesh{0}/{1}-000.txt'.format(num, sil)\n",
    "    embed_fn = '/mnt/permanent/Language/Multi/FB/wiki.{0}/wiki.{0}.vec'.format(sil2fb[sil])\n",
    "    \n",
    "    print('swad file: {}'.format(swad_fn))\n",
    "    print('embedding file: {}'.format(embed_fn))\n",
    "    logging.info('swadesh file: {}'.format(swad_fn))\n",
    "    logging.info('embedding file: {}'.format(embed_fn))\n",
    "    \n",
    "    swad, emb, nfi = get_embedding(swad_fn, swad_idx, embed_fn)\n",
    "    \n",
    "    missing_words = [w for (i, w) in enumerate(en_swad) if i in nfi]\n",
    "    logging.info('Missing words: {}'.format(missing_words))\n",
    "    \n",
    "    print(\"EMBED:\")\n",
    "    print(emb)\n",
    "    \n",
    "    # Filtered English Swadesh\n",
    "    en_swad_fil = [w for (i, w) in enumerate(en_swad) if i not in nfi]\n",
    "    en_emb_fil = np.delete(en_emb, nfi, 0)\n",
    "\n",
    "    W = np.ndarray(shape=(2, len(swad), emb.shape[1]), dtype=np.float32)\n",
    "    W[0, :, :] = en_emb_fil\n",
    "    W[1, :, :] = emb\n",
    "    T1, T, A = train(W, num_steps=50000)\n",
    "    \n",
    "    # Save translation matrix\n",
    "    trans_fn = os.path.join(trans_dir, 'eng_{}'.format(sil))\n",
    "    with open(trans_fn, 'w') as f:\n",
    "        np.save(f, T[0])\n",
    "    \n",
    "    # Calculate missing embeddings\n",
    "    en_emb_mis = np.take(en_emb, nfi, 0)\n",
    "    emb_mis = np.dot(en_emb_mis, T1[0])\n",
    "    \n",
    "    # Modify embedding\n",
    "    idx_before = nfi - range(len(nfi))\n",
    "    mod_embed = np.insert(emb, idx_before, emb_mis)\n",
    "    \n",
    "    # Save modified embedding\n",
    "    mod_embed_fn = os.path.join(embed_dir, 'eng_{}'.format(sil))\n",
    "    with open(mod_embed_fn, 'w') as f:\n",
    "        np.save(f, mod_embed)\n",
    "        \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110, 300)\n",
      "(86, 300)\n",
      "(24, 300)\n",
      "(24,)\n",
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "en_emb_mis = np.take(en_emb, nfi, 0)\n",
    "print(en_emb.shape)\n",
    "print(emb.shape)\n",
    "print(en_emb_mis.shape)\n",
    "print(emb_mis.shape)\n",
    "print(T[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(mod_embed_fn) as f:\n",
    "    a = np.load(f)\n",
    "mod_embed.shape\n",
    "\n",
    "trans_fn = os.path.join(trans_dir, 'eng_{}'.format(sil))\n",
    "with open(trans_fn, 'w') as f:\n",
    "    np.save(f, T[0])\n",
    "\n",
    "with open(trans_fn) as f:\n",
    "    b = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00102911 -0.01292466 -0.02002739 -0.07683681 -0.01574634  0.12186155\n",
      " -0.0443708  -0.09487347  0.0615467  -0.02135201  0.08542475  0.06812111\n",
      "  0.06824581  0.00152703 -0.00650354 -0.04498217  0.05246906 -0.05029584\n",
      " -0.0375698  -0.1642462   0.11949519 -0.00243541 -0.03330092 -0.00301179\n",
      "  0.04701243  0.08603003  0.09571903 -0.10634333  0.13593654 -0.05885337\n",
      "  0.10986245 -0.02022966 -0.02585813  0.04824732  0.05251316 -0.07307283\n",
      " -0.04734549 -0.07056655 -0.12404237 -0.10819566  0.02274202 -0.017001\n",
      "  0.01467388 -0.0551046  -0.0472588  -0.03024867 -0.02453047 -0.119883\n",
      "  0.02405598 -0.0824379   0.01190526 -0.01121893  0.03161435  0.01167349\n",
      "  0.00277546  0.03274126 -0.13349566 -0.01576459  0.05750442  0.01746485\n",
      "  0.02760097 -0.00511384  0.02710366  0.04140221 -0.0013046  -0.00644241\n",
      "  0.05162653  0.02720708  0.00279355  0.04033613  0.01744964 -0.10749914\n",
      " -0.00332036 -0.06976509 -0.05806103  0.0842507   0.03002208  0.05475178\n",
      "  0.04726032 -0.03520191 -0.01965784 -0.11635474  0.10911878 -0.04045779\n",
      "  0.04703524 -0.10858651  0.04662767  0.05637142  0.07492516 -0.08212614\n",
      "  0.01241154 -0.07589239 -0.00493377  0.04756144 -0.00521375 -0.01936737\n",
      "  0.00996184 -0.0527139  -0.00044882 -0.09532971  0.01717285 -0.02232836\n",
      " -0.06302948 -0.01162529  0.07052093 -0.0594921  -0.04414573  0.05844579\n",
      "  0.05293138 -0.01661624 -0.08140224 -0.04956281 -0.01009962  0.00794404\n",
      " -0.09482024  0.02652576  0.09287818 -0.06000613 -0.03745117  0.06042435\n",
      "  0.12622321 -0.01093074 -0.02698808  0.05170561  0.05374045  0.08583993\n",
      "  0.03388034  0.02066613 -0.00222599  0.00165113  0.02867769 -0.01818418\n",
      "  0.092682   -0.00672451 -0.03653261  0.06686341  0.05034602 -0.08588555\n",
      "  0.02215651  0.00619391  0.00529998 -0.08705808 -0.06974684 -0.01042674\n",
      "  0.06911571 -0.00771258  0.01623756 -0.04577602  0.05717441 -0.03777967\n",
      "  0.02017187  0.01273319 -0.02964644  0.01030721 -0.02170027  0.09665584\n",
      " -0.05456167  0.10195126 -0.12742767 -0.08836293 -0.07704364  0.0928721\n",
      " -0.00451662 -0.07033996  0.0061822   0.01306594  0.09742233 -0.08007002\n",
      "  0.04003045  0.12750524  0.07208127  0.00682823  0.02887996  0.00950985\n",
      " -0.05362638  0.02306747 -0.02506579 -0.08539738  0.05876973 -0.03142425\n",
      "  0.04740632  0.0860346  -0.06109807 -0.02880696 -0.04525439  0.02802223\n",
      " -0.048407    0.00546529  0.07083877 -0.04018405  0.0210372   0.12962067\n",
      "  0.07110187  0.00559927  0.10325915 -0.0833002  -0.05340739 -0.00994359\n",
      " -0.08426438  0.03315948  0.13866486  0.02310701  0.02649078  0.02026007\n",
      " -0.01107369 -0.02607865  0.03245687 -0.01861913 -0.04600719  0.02116647\n",
      " -0.02162879 -0.0837975   0.04546578  0.02541406 -0.01976429  0.01265471\n",
      " -0.01827087  0.04049885  0.02606648 -0.05941302 -0.0865471  -0.02954455\n",
      "  0.09489323 -0.14303868 -0.01313255  0.01567334 -0.01738272  0.04154668\n",
      " -0.09367508 -0.03803821  0.08999171 -0.04024792  0.02211392 -0.00212121\n",
      "  0.05750746  0.04840548 -0.01873623  0.05566425  0.06802835 -0.04846631\n",
      "  0.07334657 -0.00666794 -0.02735764  0.00451875  0.02167289  0.04116344\n",
      "  0.03039315  0.06354807 -0.01315551  0.02849367  0.02215651 -0.01896739\n",
      " -0.01147701 -0.04828838 -0.09946019 -0.0947655  -0.03102732  0.03761846\n",
      "  0.05802149  0.05354274  0.03300284  0.02543687  0.04293213 -0.02858948\n",
      " -0.04646647 -0.04670219 -0.00409064 -0.04299753  0.02657899  0.01254887\n",
      " -0.06155279 -0.01704358  0.00206813  0.10451837 -0.03233977 -0.00390358\n",
      "  0.03719264  0.10902906  0.02331992  0.08024339 -0.07544679  0.06223562\n",
      "  0.04613797  0.00265714 -0.05155505  0.01824045  0.06486812  0.12349185\n",
      " -0.04785648  0.01953313  0.04152691 -0.06307814 -0.032174    0.02446204\n",
      "  0.04566044  0.06289565  0.00314288  0.05754244  0.03279753 -0.03332069]\n",
      "[-0.00102911 -0.01292466 -0.02002739 -0.07683681 -0.01574634  0.12186155\n",
      " -0.0443708  -0.09487347  0.0615467  -0.02135201  0.08542475  0.06812111\n",
      "  0.06824581  0.00152703 -0.00650354 -0.04498217  0.05246906 -0.05029584\n",
      " -0.0375698  -0.1642462   0.11949519 -0.00243541 -0.03330092 -0.00301179\n",
      "  0.04701243  0.08603003  0.09571903 -0.10634333  0.13593654 -0.05885337\n",
      "  0.10986245 -0.02022966 -0.02585813  0.04824732  0.05251316 -0.07307283\n",
      " -0.04734549 -0.07056655 -0.12404237 -0.10819566  0.02274202 -0.017001\n",
      "  0.01467388 -0.0551046  -0.0472588  -0.03024867 -0.02453047 -0.119883\n",
      "  0.02405598 -0.0824379   0.01190526 -0.01121893  0.03161435  0.01167349\n",
      "  0.00277546  0.03274126 -0.13349566 -0.01576459  0.05750442  0.01746485\n",
      "  0.02760097 -0.00511384  0.02710366  0.04140221 -0.0013046  -0.00644241\n",
      "  0.05162653  0.02720708  0.00279355  0.04033613  0.01744964 -0.10749914\n",
      " -0.00332036 -0.06976509 -0.05806103  0.0842507   0.03002208  0.05475178\n",
      "  0.04726032 -0.03520191 -0.01965784 -0.11635474  0.10911878 -0.04045779\n",
      "  0.04703524 -0.10858651  0.04662767  0.05637142  0.07492516 -0.08212614\n",
      "  0.01241154 -0.07589239 -0.00493377  0.04756144 -0.00521375 -0.01936737\n",
      "  0.00996184 -0.0527139  -0.00044882 -0.09532971  0.01717285 -0.02232836\n",
      " -0.06302948 -0.01162529  0.07052093 -0.0594921  -0.04414573  0.05844579\n",
      "  0.05293138 -0.01661624 -0.08140224 -0.04956281 -0.01009962  0.00794404\n",
      " -0.09482024  0.02652576  0.09287818 -0.06000613 -0.03745117  0.06042435\n",
      "  0.12622321 -0.01093074 -0.02698808  0.05170561  0.05374045  0.08583993\n",
      "  0.03388034  0.02066613 -0.00222599  0.00165113  0.02867769 -0.01818418\n",
      "  0.092682   -0.00672451 -0.03653261  0.06686341  0.05034602 -0.08588555\n",
      "  0.02215651  0.00619391  0.00529998 -0.08705808 -0.06974684 -0.01042674\n",
      "  0.06911571 -0.00771258  0.01623756 -0.04577602  0.05717441 -0.03777967\n",
      "  0.02017187  0.01273319 -0.02964644  0.01030721 -0.02170027  0.09665584\n",
      " -0.05456167  0.10195126 -0.12742767 -0.08836293 -0.07704364  0.0928721\n",
      " -0.00451662 -0.07033996  0.0061822   0.01306594  0.09742233 -0.08007002\n",
      "  0.04003045  0.12750524  0.07208127  0.00682823  0.02887996  0.00950985\n",
      " -0.05362638  0.02306747 -0.02506579 -0.08539738  0.05876973 -0.03142425\n",
      "  0.04740632  0.0860346  -0.06109807 -0.02880696 -0.04525439  0.02802223\n",
      " -0.048407    0.00546529  0.07083877 -0.04018405  0.0210372   0.12962067\n",
      "  0.07110187  0.00559927  0.10325915 -0.0833002  -0.05340739 -0.00994359\n",
      " -0.08426438  0.03315948  0.13866486  0.02310701  0.02649078  0.02026007\n",
      " -0.01107369 -0.02607865  0.03245687 -0.01861913 -0.04600719  0.02116647\n",
      " -0.02162879 -0.0837975   0.04546578  0.02541406 -0.01976429  0.01265471\n",
      " -0.01827087  0.04049885  0.02606648 -0.05941302 -0.0865471  -0.02954455\n",
      "  0.09489323 -0.14303868 -0.01313255  0.01567334 -0.01738272  0.04154668\n",
      " -0.09367508 -0.03803821  0.08999171 -0.04024792  0.02211392 -0.00212121\n",
      "  0.05750746  0.04840548 -0.01873623  0.05566425  0.06802835 -0.04846631\n",
      "  0.07334657 -0.00666794 -0.02735764  0.00451875  0.02167289  0.04116344\n",
      "  0.03039315  0.06354807 -0.01315551  0.02849367  0.02215651 -0.01896739\n",
      " -0.01147701 -0.04828838 -0.09946019 -0.0947655  -0.03102732  0.03761846\n",
      "  0.05802149  0.05354274  0.03300284  0.02543687  0.04293213 -0.02858948\n",
      " -0.04646647 -0.04670219 -0.00409064 -0.04299753  0.02657899  0.01254887\n",
      " -0.06155279 -0.01704358  0.00206813  0.10451837 -0.03233977 -0.00390358\n",
      "  0.03719264  0.10902906  0.02331992  0.08024339 -0.07544679  0.06223562\n",
      "  0.04613797  0.00265714 -0.05155505  0.01824045  0.06486812  0.12349185\n",
      " -0.04785648  0.01953313  0.04152691 -0.06307814 -0.032174    0.02446204\n",
      "  0.04566044  0.06289565  0.00314288  0.05754244  0.03279753 -0.03332069]\n"
     ]
    }
   ],
   "source": [
    "print(a[7])\n",
    "print(emb[6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
